<html>
<head>
<meta name="description" content="James Hazelden">
<link rel=StyleSheet href="nut_styles.css" type="text/css" media=all>
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body>
<h1>Finite Difference Gradients in Neuronal Circuits</h1>
<hr>
<p>
Using Pytorch to compute gradients explicitly in circuits of realistic complex neuronal models, especially those with very complex internal connections, can be very computationally expensive. Furthermore, I think that issues such as numerical stability will become significant because of all the multiplications over every timestep, among other factors. One idea is to compute gradients using something different to backpropogation. In particular, I tried out using finite difference methods instead, which, as far as I can tell online, is not a particularly well-researched idea, especially in terms of actual applications.
</p>
<p>
The idea is simple, suppose we have a function:
$$f: A \rightarrow B, x \mapsto y.$$
Then, to measure the gradient of \(f\) with respect to \(x\), we can approximate it using a small perturbation \(\Delta x\):
$$\frac{\partial f}{\partial x} \approx \frac{f(x + \Delta x) - f(x)}{\Delta x}.$$
Now, suppose we have some network of neurons, for example a fully connected network of 5 neurons with weights:
</p>
<img src="./dot_1.png" alt="Net 1"/>
<p>
Suppose each neuron has some internal state \(V\) and some associated output \(a\) (these are dependent on time but I will not mention that for now). Let us denote the weighted output of all neurons as \(z\). So, if we put the weights into a 5 by 5 matrix \(W\), then 
$$z = W a$$
I think in most real networks, the matrix \(W\) will be sparse. Now, suppose we have some function \(f: \mathbb{R}^5 \rightarrow \mathbb{R}\) that quantifies the output into a single scalar. Suppose we want to measure how the weights affect \(f(z) = f(W a)\). At first, it might seem that we need to measure all weights' influence on this quantity. However, as in traditional backpropogation, we can actually just measure the influence of the weighted input \(z\) on \(f\) and then "interpolate" to the individual weights by an outer product. In particular, by the chain rule,
$$D_{_W} f(z) = (\nabla f(z))^T \cdot D_{_W} z = (\nabla f(z))^T \cdot a^T,$$
so we just measure the left gradient \(\nabla f(z)\) (which, before the transpose, is a row vector), then take an outer product with the unweighted output \(a\). 
</p>
<p>
Next, we compute the gradient using finite-difference methods. In particular, we compute the individual partial derivatives \(\partial f / \partial z_i\) which compose the gradient. This is done by the following:
$$\frac{\partial f}{\partial z_i} \approx \frac{f(z_i + \Delta z_i) + f(z_i)}{\Delta z_i}.$$ 
The incremement \(\Delta z_i\) should be small, and I'm not sure what is a good criterion for choosing it exactly. My mind turns to grid computations and other stuff when I see the rule above but I haven't fleshed out those ideas yet. 
</p>

<p>
<b>In summary:</b> to compute the total derivative \(D_{_W} f\), we first approximate \(\nabla f(z)\) as above, then we take an outer product with \(a\). 
</p>

<h2>Simple Example</h2>

<p>TODO: text goes here</p>


<h2>Network of Coupled Oscillators</h2>

I wanted to use a simple oscillator model to try out these ideas in a more specific application and see how we do.


<h2>Mathematical Details</h2>

In here we will go through the details of stability/order of accuracy/computational complexity/etc.

<br><br><br><br><br><br>
</td></tr></table></center>
</body>

</html>

