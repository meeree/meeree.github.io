<html>
<head>
<meta name="description" content="James Hazelden">
<!--<link rel=StyleSheet href="pdbstyle.css" type="text/css" media=all>-->
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body>
<h1>Finite Difference Gradients in Neuronal Circuits</h1>
<p align="justify">
Using Pytorch to compute gradients explicitly in circuits of realistic complex neuronal models, especially those with very complex internal connections, can be very computationally expensive. Furthermore, I think that issues such as numerical stability will become significant because of all the multiplications over every timestep, among other factors. One idea is to compute gradients using something different to backpropogation. In particular, I tried out using finite difference methods instead, which, as far as I can tell online, is not a particularly well-researched idea, especially in terms of actual applications.</p>
<p>
The idea is simple, suppose we have a function:
$$f: A \rightarrow B, x \mapsto y.$$
Then, to measure the gradient of $f$ with respect to $x$, we can approximate it using a small perturbation $\Delta x$:
$$\frac{\partial f}{\partial x} \approx \frac{f(x + \Delta x) - f(x)}{\Delta x}.$$
</p>
</p>

<br><br><br><br><br><br>
</td></tr></table></center>
</body>

</html>

