<html style="background: url(background_neuromod.png);
      background-size: repeat; 
      background-size: 40%;
	  zoom: 0.45;
>
<head>
<meta name="description" content="James Hazelden">
<link rel=StyleSheet href="nut_styles.css" type="text/css" media=all>
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<link rel="icon" href="./koch-snowflake.png" type="image/x-icon">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body style="">
<div class="main_text">
<h1 id="top">Isolated Contributions of <span style="color:#777;">Neuromodulators</span> on Gradient Descent Learning Using <span style="color:#777;">KP-Flow</span></h1>
<hr>

<table style="margin-left: auto; margin-right: auto; width: 80%;font-size:.8cm; border-collapse:separate; border:solid black 3px; border-radius:6px;">

<tr>
<td  colspan=2 style="background-color: white; ">
<b>Relevant Links</b>
</td>
</tr>

<tr>
<td style="width:50%; background-color: #eeeeee;">
<a href = "kpflow.html">Prior Article</a>
</td>

<td  style="width:50%; background-color: #eeeeee;">
<a href = "https://github.com/meeree/kpflow/">Code for This Blog</a>
</td>
</tr>


</table>


<br>

<p class = "center_text">
<u>TLDR</u><br>
<i><b>Q: </b>Neuro-modulators can dynamically "freeze" synaptic weights during learning. Can this improve learning?</i><br>
<i><b>A: </b>We can better isolate the effect of freezing weights in recurrent networks with the K operator of KP-Flow.</i>
</p>

<br>
<br>



<p class="center_text">
<b style="font-size: .89cm;">Synopsis</b><br>
This blog post investigates the effect of adding neuro-modulator like mechanisms into deep learning frameworks. 
This is a fancy way of saying that (as a preliminary step) I'll investigate freezing specific recurrent model weights during training, seeing how this affects learning. 
The nice thing is that, using my framework in the prior blog, the only operator that changes during Gradient Descent (GD) learning is \(\mathcal{K}\), so I can use that to explictily guess what will happen to the GD flow. 
</p>
<br>
<br>

<br>
<hr>

<h1 id="setup">Setup</h1>
<br>
<p>
<b>Setup</b> Consider a simple vanilla discrete RNN with dynamics
$$h(t+1) = f(h(t), x(t), \{W\}) = W \sigma(h(t)) + W_{in} x(t), \, \text{ With } \, h(0) = 0.$$
We'd like to train this model to match a target \(y^*(t)\), associate with each given input \(x(t)\) from some dataset provided. 
Specifically, we define the output of the model, \(y(t)\), by 
$$y(t) := W_{out} h(t) + b_{out},$$
and we'd like to minimize some average loss 
$$L := \langle \ell(y(t), y^*(t)\rangle_{x,t}.$$ 
For simplicity <i>I'll assume \(W\) is the only trainable parameter</i>, which will be iteratively tuned by Gradient Descent (GD). 
Note that the choice of model, parameters, etc., can actually be made very general in my KP-Flow formulation, and I just made these ones for now as an example.
</p>

<p>
<b>The \(\mathcal{K}\) Operator</b> With the formulaton above, the \(\mathcal{K}\) operator (see prior blog post above) corresponds to the ''covariance operator'' of the recurrent activity:
$$\mathcal{K} = H \cdot \sigma(h) \otimes \sigma(h)^T.$$
More specifically, the action of \(\mathcal{K}\) on any tensor \(q\) of shape [B, T, H] (over batch inputs, feed-forward times and hidden unit dimension) can be written as 
$$(\mathcal{K} q)(t, x) = H \cdot \langle (\sigma(h(t, x)^T \sigma(h(t_0, x_0)))) \cdot q(t_0, x_0) \rangle_{t_0, x_0},$$
where note the notation \(h(t, x)\) means the value of the hidden state at time \(t\) on the trial with the particular input \(x\) specified (note \(x\) here is basically an index in the range 0 to B, the batch size). 
</p>

<p>
In other words, \(\mathcal{K} q (t,x)\) takes a weighted sum over all of \(q\)'s entries, scaling by the inner product of how much \(\sigma(h)\) at each time and trial aligns with \(\sigma(h(t,x))\) at the exact time \(t\) and exact input \(x\).
Formally speaking, this operator is a Hilbert-Schmidt integral operator with kernel given by the inner products of \(\sigma(h)\), hence why it is called the ''covariance operator.'' 
Importantly, the effective latent dimension of the activity, \(\sigma(h)\), is identical to the effective rank of \(\langle \mathcal{K} \rangle_{x,t}\), the operator \(\mathcal{K}\) reduced over time and trials.
</p>
<!--
<h2 id="freeze">Randomly Freezing Recurrent Synapses</h2>

<p>
Now the setup is done, let's consider the effect on the operator \(\mathcal{K}\) when some of the weight parameters, \(w_{ij}\), of the weight matrix, \(W\), are frozen, meaning that we keep them static and don't apply a GD update at a particular iteration to them. 
Note, the nice thing about my KP-Flow formulation is that \(\mathcal{P}\) <i>stays completely the same</i> (at a single GD iteration) irregardless of the choice of frozen/non-frozen parameters, only \(\mathcal{K}\) changes. 
</p>
<p>
The operator \(\mathcal{K}\) is defined in terms of the parameter Jacobian of the one-step dynamics, \(J_\theta := \text{d} f / \text{d} \theta\), where \(\theta\) are only the parameters we train.  
The initial parameters are \(\theta := \text{vec}(W) \in \mathbb{R}^{H^2}\), so \(M = H^2\). Let 
$$S = \{s_1, ..., s_K\} \,\, \text{ Where all } s_i \leq M, $$
denote a set of indexed synaptic connections we want to train. Then, define the <i>free parameters</i>, \(\theta_S\), by 
$$\theta_S = [\theta_{s_1}, \theta_{s_2}, \dots, \theta_{s_K}]^T \in \mathbb{R}^K$$ 
What is the parameter kernel \(\mathcal{K}_S\) corresponding to only training \(\theta_S\)? Clearly,
$$\frac{\text{d} f}{\text{d} \theta_S} = (\frac{\text{d} f}{\text{d} \theta})_S = [\frac{\text{d} f}{\text{d} \theta_{s_1}}, \frac{\text{d} f}{\text{d} \theta_{s_2}}, \dots, \frac{\text{d} f}{\text{d} \theta_{s_K}}]^T, $$ 
i.e. the original jacobian, of shape [B, T H, M], but keeping only entries in \(S\), which is now of shape [B, T, H, K]. 
</p>
-->


<h2 id="gen">General Formulation</h2>

<p>
Consider replacing the update
$$\delta \theta = - \nabla_\theta L$$
with the synapse-adapted update 
$$\delta \theta = - Q(\nabla_\theta L),$$ 
where \(Q : \mathbb{R}^M \rightarrow \mathbb{R}^M\) is a linear function on the parameter space (it doesn't really have to be linear). More specifically, the choice
$$\text{Example: } Q(\nabla_\theta L) = v \odot \nabla_\theta L,$$ 
for some vector \(v \in \mathbb{R}^M\) corresponds to freezing synapses if \(v\) is a binary vector, or, more practically, can model adaptive learning rate rules like AdaGrad that use second-order Hessian-like information, in which case \(Q\) is also a function of the state at this or previous iterations.
The full GD flow becomes
$$\delta z = (\mathcal{P} J_\theta M J_\theta^* \mathcal{P}^*)(\text{Err}),$$
i.e. \(\mathcal{K} = J_\theta \otimes_M J_\theta^*\) is replaced by 
$$\mathcal{K}_M := J_\theta \otimes_M Q \otimes_M J_\theta^*$$
As a reminder, the shape of \(J_\theta\) is [B, T, H, M] and Q is a matrix of shape [M, M] and the notation \(\otimes_M\) means "contract along the \(M\) dimension." So, \(Q \otimes_M J_\theta^*\) is a tensor of shape [M, H, T, B] and thus \(J_\theta \otimes_M Q \otimes_M J_\theta^*\) is a tensor of shape [B, T, H, H, T, B], i.e. a linear operator on [B, T, H], as desired :)
</p>

<hr  id="exmp">
<p class="center_text" style="padding-top:0;"><b>A Worked Example</b></p>
<br>
<p>
<b>Defining \(Q\):</b> Consider a specific case of the Hadamard binary matrix example. Define \(P_1, P_2\) to be H-by-H diagonal binary matrices so that \(P_1\) has R ones on the diagonal, \(P_2\) has C ones on the diagonal. Define a linear mapping \(Q_{mat}\) 
$$Q_{mat}(W) := P_1^T W P_2,$$
which maps \(\mathbb{R}^{H \times H}\) to \(\mathbb{R}^{H \times H}\), <i>selecting only R rows and C columns to not zero-out/freeze based on the non-zero diagonal entries of \(P_1, P_2\)</i>. 
Let's finally define \(Q\) as a map on \(\mathbb{R}^M = \mathbb{R}^{H^2}\), the vectorized space. 
This is done using the <a href="https://en.wikipedia.org/wiki/Vectorization_%28mathematics%29?utm_source=chatgpt.com#Compatibility_with_Kronecker_products">Krocker product identity</a>, yielding
$$Q \theta := \text{kron}(P_2, P_1)^T \theta,$$
where kron takes matrices of shape \(\mathbb{R}^{p, q}, \mathbb{R}^{\ell, m}\) and produces a matrix of shape \(\mathbb{R}^{p \ell, q m}\). So, \(Q\) is represented by the H\(^2\) by H\(^2\) matrix \(\text{kron}(P_2, P_1)^T\). 
We can verify that \(Q \text{vec}(W) = \text{vec}(Q_{mat}(W))\). 
I adopted the notation kron instead of \(\otimes\) because it's just confusing to me to mix that with the tensor product above, even though it may technically agree.
</p>

<p>
<b>Resulting \(\mathcal{K}_Q\):</b> For the RNN, as above, 
$$J_\theta = \sigma \otimes \text{vec}(\text{Id}_{H \times H}),$$
which is a tensor of shape [B, T, H, M] = [B, T, H, H\(^2\)]. Then,
$$Q \otimes_M J_\theta^* = \text{vec}(P_1^T P_2) \otimes \sigma^T.$$ 
Note here \(\sigma^T\) is thought to be shape [H, T, B]. Finally, note \(\text{vec}(\text{Id}_{H \times H}) \otimes_M \text{vec}(P_1^T P_2)\) is just the trace, \(\text{Tr}(P_1^T P_2)\), so \(\mathcal{K}_Q\) is just a scalar multiple of \(\mathcal{K}\):
$$\mathcal{K}_Q = J_\theta \otimes_M Q \otimes_M J_\theta^*$$
$$= \text{Tr}(P_1^T P_2) \cdot (\sigma \otimes \sigma^T).$$
Now, recall \(P_1, P_2\) are just diagonal, binary matrices, so \(\text{Tr}(P_1^T P_2) = \# \{i | P_1[i,i] =1 \text{ and } P_2[i,i] = 1\}\), i.e. the <i>number of common columns and rows selected</i>. Note neurons selected with \(P_1\) will have 
</p>

<figure>
<a href="K7.png"><img src="K7.png" width=50%/></a>
<figcaption><p><b>Fig 1</b> Example network, where \(P_1\) specifies the red and green neurons, \(P_2\) specifies the blue and green neurons and \(\text{Tr}(P_1^T P_2) = 2\).</p></figcaption>
</figure>

<br>
<hr>

<!--
<p>
MORE GENERAL: By the chain rule,
$$\frac{\text{d} f}{\text{d} \theta_S} = \frac{\text{d} f}{\text{d} \theta} \otimes_{M} \frac{\text{d} \theta}{\text{d} \theta_S}$$ 
Note \(\frac{\text{d} f}{\text{d} \theta}\) is shape [B, T, H, M] and \(\frac{\text{d} \theta}{\text{d} \theta_S}\) is shape [M, K], so the operation above means "contract out the M dimension" (see my <a href="tensor_calc.html">blog post</a> on tensor calculus :). 
This makes sense, since the resulting quantity has shape [B, T, H, K]. The Jacobian matrix \(\frac{\text{d} \theta}{\text{d} \theta_S}\) is just a binary matrix with ones in the indices corresponding to \(S\), zeros elsewhere.
</p>
-->




<!-- Navbars -->
<a href="index.html" style="text-decoration: none;">
<div class="top_bar" style="left: 30px;">
goto: main
</div>
</a>

<a href="" style="text-decoration: none;">
<div class="top_bar" style="right: 30px;">
goto: top
</div>
</a>

<div class="top_bar" style="top: 4vw; left: 4vw; bottom: auto; width: 6.5vw; height: 6.5vw; 
    border-radius: 1px; border: 0;  image-rendering: pixelated; border: 1px solid grey;">
	
<label for="toggle" id="toggle-btn"  class="toc-clicker">
<img style="width: 100%; height: 100%;" src="toc.png">
</label>

<input type="checkbox" id="toggle" hidden>

<div id="toc" class="top_bar" style="top: 10.5vw; left: 4vw; bottom: auto; width: 15%; 
height: 30%;  /* ADJUST */
border-radius: 0; border: 0;  image-rendering: pixelated; border: 0; background: none;">

<table class = "toctable" style="width: 100%; height: 100%; padding: 0; background: white; border-radius: 0; margin-top: 10px;">

<tr><td style = "text-align: center; color: black;">
<a href="#top">
Isolated Contributions of Neuromodulators
</a>
</td></tr>

<tr><td>
<a href="#setup">
Setup
</a>
</td></tr>

<tr><td>
<a href="#gen">
General Formulation
</a>
</td></tr>

<tr><td>
<a href="#exmp">
A Worked example
</a>
</td></tr>

</table>
</div>
</div>



</body>
</html>
