<!--- 1. Hidden NTK vs Output NTK and two terms
2. Final time NTK vs all times -->

<html style="background: url(art/modes_poster.png);
      background-size: repeat; 
      background-size: 50%;">
<head>
<meta name="description" content="James Hazelden">
<link rel=StyleSheet href="nut_styles.css" type="text/css" media=all>
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<link rel="icon" href="./koch-snowflake.png" type="image/x-icon">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body>

<div class="main_text">
<h1>Efficient NTK Metrics for General Recurrent Models</h1>
<hr>

<p>
This blog post explores computing NTK metrics efficiently for recurrent models, using the operators in our pre-print <a href = "https://arxiv.org/abs/2507.06381">here</a>. Find related code in  <a href ="https://github.com/meeree/kpflow/blob/main/src/kpflow/trace_estimation.py">my package</a>.
</p>

<h2 id="background">Background</h2>

<p>
Here, I'll briefly describe the mathematical preliminaries. For a more detailed introduction, see the Appendix of the pre-print above.
</p>
<br>

<p>
The aim is to train a dynamical system to minimize some loss over a set of trainining inputs. Every quantity considered thus has a time axis, \(t\), which we assume is in a fixed window \(t \in [0, t_{end}]\) for simplicity. 
Specifically, we are given a <i>training set</i> \(X\) consisting of inputs for each <i>trial</i> \(x(t) \sim X\). The model has hidden state \(z(t, x)\) on trial input \(x\) and at time \(t\), with dynamics given by a parameterized ODE: 
$$\frac{d}{dt} z(t,x) = f(z(t,x), t, x, \theta); \, \, \text{ where } z(0, x) = z_0.$$ 
The parameters are \(\theta\), which evolve according to Gradient Descent (GD), as below. All trajectories have initial condition \(z_0\) for simplicity. The function \(f\) thus describes the tangential flow of the model dynamics.
</p><p>
Given this setup, there is assumed to be a loss function \(\ell(z(t,x))\) quantifying how far we are from some objective (typically involving a target \(y^*(t, x)\) for supervised problems, along with output weights).
The goal is to minimize the average loss over all time and trials:
$$\text{ Goal:  } \, \arg \min_\theta L := \arg \min_\theta \langle \ell(z(t,x)) \rangle_{t,x},$$
where \(\langle \cdot \rangle_{t,x}\) denotes averaging over \(t, x\). To solve this problem, we can use GD, evolving \(\theta\) along the steepdest descent direction in parameter space. I.e., we incrementally perturb the parameters by:
$$\delta \theta = -\nabla_\theta L.$$
When we take infinitesimal steps along these updates, we define the so-called <i>gradient flow</i> of the parameters, \(\theta\). 
</p><p>
However, it is useful to consider the gradient flow of other quantities, since the parameters are ultimately a proxy for the model dynamics themselves. For example, we can investigate how the model state, \(z(t,x)\), evolves under GD.
Technically, this quantity is a tensor over all trial inputs, \(x\), times, \(t\), and hidden units. In the discrete setting with B = # of batches, T = # of timesteps, H = # of hidden units, then \(z\) is a three-tensor of shape [B,T,H].
</p><p>
We let \(\mathbb{T}\) denote the space of all such tensors (discrete or continuous) throughout. As in my paper, there are two linear operators, \(\mathcal{P}, \mathcal{K} :\mathbb{T} \rightarrow \mathbb{T}\), such that 
$$\delta z = \mathcal{P K P^*} (\text{Err}),$$
describes the flow of error corrections, \(\text{Err} = \nabla_z \ell\), to perturbations \(\delta z\), i.e. the <i>hidden state GD flow</i>. 
The operator \(\Theta := \mathcal{P K P^*}\) is thus a more-general anologue of the <i>Neural Tangent Kernel</i> (NTK) for recurrent dynamical systems. Classically, the NTK describes evolution of the output of a neural network with a scalar output and no notion of time, so it is a matrix over batch trials. 
In our case, however, the NTK is an operator on tensors in \(\mathbb{T}\), of shape [B, T, H] when discretized, i.e. it is a linear operator on three-tensors. 
</p>

<h2 id="motiv">Motivation</h2>

<p>
Prior work has used the NTK to quantify <i>rich versus lazy</i> learning. Rich learning occurs when the hidden state representation changes a lot after we train the model with GD. This can be summarized basically as:
$$\text{ NTK Changes a Lot } \Rightarrow \text{ Rich Learning}; \text{ NTK Basically Constant } \Rightarrow \text{ Lazy Learning}.$$
For example, for feed-forward neural networks, it has been shown that when the number of hidden units goes to infinity they fall into the lazy regime, where the NTK is completely static throughout training. 
</p><p>
To quantify this, we define the <i>NTK alignment</i> as in <a href = "https://arxiv.org/abs/2310.08513">this paper</a> Equation 6. Letting \(\Theta_0, \Theta_f\) denote the operator \(\mathcal{P K P^*}\) before and after GD training on a recurrent model, the alignment is given by:
$$\text{KA}(\Theta_0, \Theta_f) = \frac{\text{Tr}(\Theta_0^* \Theta_f)}{\sqrt{\text{Tr}(\Theta_0^* \Theta_0) \text{Tr}(\Theta_f^* \Theta_0)}},$$ 
where Tr computes the trace of the linear operator. Note that when, for example, [B, T, H] = [100, 90, 256], which is a very small network with very few batches, the discretized operator \(Theta\) can be seen as a matrix with 2,304,000 rows and columns! <b><u>In this article, I will focus on efficiently computing this expression</u></b>.
</p>

<h2 id="naive">Naive Approach</h2>

<p>
Naively, we can explictly compute \(\Phi\) as an outer product of parameter derivatives. In particular, \(\Phi\) is a tensor-product of the all parameter derivatives of the hidden state, \(z\). We can thus construct it as follows:
$$J_\theta[b, t, i] := \text{autograd}(z[b, t, i], \theta).$$
This is a tensor of shape [B, T, H, M], assuming there are M parameters. Then, \(\Phi\) is a six-tensor of shape [B, T, H, B, T, H] defined by 
$$\Phi[b_0, t_0, i_0, b_1, t_1, i_1] = J_\theta[b_0, t_0, i_0]^T J_\theta[b_1, t_1, i_1].$$
It is clear that this approach is super inefficient. Naively, this will require B * T * H evaluations of autograd to form the parameter Jacobian. 
</p> <p>
However, if we are interested in small problems and consider objects other than the state \(z\), it can be feasible. For example, in Helena Liu's paper above, the NTK of the output of the network at the final timestep is considered. 
This operator is much smaller than the NTK of the state at every single timestep. In that case, the Naive approach can be used, as in her code, which I'll reproduce here:
</p>

<br>

<code style="width:100%;"> <b>Naive Approach:</b>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%;"><span></span>J <span style="color: #333">=</span> [] <span style="color: #888">  # J_theta, parameter jacobian [B*T*H, M].</span>
<span style="color: #080; font-weight: bold">for</span> b <span style="color: #000; font-weight: bold">in</span> <span style="color: #007020">range</span>(batch_size):
  <span style="color: #080; font-weight: bold">for</span> t <span style="color: #000; font-weight: bold">in</span> <span style="color: #007020">range</span>(time_size):
    <span style="color: #080; font-weight: bold">for</span> k <span style="color: #000; font-weight: bold">in</span> <span style="color: #007020">range</span>(hidden_size): 
      J_i <span style="color: #333">=</span> torch<span style="color: #333">.</span>unsqueeze(torch<span style="color: #333">.</span>autograd<span style="color: #333">.</span>grad(hidden[b,t,k], \
        net<span style="color: #333">.</span>params, retain_graph<span style="color: #333">=</span><span style="color: #080; font-weight: bold">True</span>)[<span style="color: #00D; font-weight: bold">0</span>], dim<span style="color: #333">=</span><span style="color: #00D; font-weight: bold">0</span>) <span style="color: #888"># net.params is a length M vec.</span>
      J<span style="color: #333">.</span>append(J_i)
J <span style="color: #333">=</span> torch<span style="color: #333">.</span>stack(J, <span style="color: #00D; font-weight: bold">0</span>)
Theta <span style="color: #333">=</span> J <span style="color: #333">@</span> J<span style="color: #333">.</span>T <span style="color: #888">  # Form the NTK Theta Matrix.</span>
</pre></div>
</code>
<br>

<h2 id="matrixfree"> Matrix-Free Approaches</h2>

<p>
The Naive approach explictly computes \(J_\theta\), which is very intractable, both in memory and compute. In the following sections, the methods introduced are <i>matrix-free</i>, relying only on applications of the operator \(\Theta\) to specific vectors, never actually computing it explictly.
</p>

<h3>Preliminary: Computing Actions with VJP and JVP</h3>

<p>
As in <a href="https://docs.pytorch.org/tutorials/intermediate/neural_tangent_kernels.html">this</a> tutorial, this action can be efficiently computed using built-in pytorch (or jax) functionality: <i>vjp</i> and <i>jvp</i>. 
Specifically, vjp (Vector-Jacobian Product) takes in a tensor of shape [B, T, H] and returns a quantity in the parameter space of shape [M] (in reality the parameters are typically a named dictionary, which we can vectorize to make a vector). 
In particular, assuming \(J_\theta\) has flattened shape [B*T*H, M] and \(v\) has shape [B*T*H], then
$$\text{vjp}(v) = J_\theta^T \, v.$$ 
Similarly, jvp (Jacobian-Vector Product) takes in a quantity, \(\phi\), which is a vector of shape [M] in the parameter space, and outputs a quantity of shape [B*T*H]:
$$\text{jvp}(\phi) = J_\theta \, \phi.$$
These operations actually implicitly compute the products using automatic differentiation, without forming \(J_\theta\). Firstly, vjp uses backwards-mode automatic differentiation, i.e. backpropagation, while jvp uses forward-mode automatic differentiation, which is more memory intensive. 
</p>

<h3>Hutch++ and Nystrom++</h3>

<p>
In my code, I implement the linear operators \(\mathcal{P}, \mathcal{K}\) as classes that can be converted into scipy <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.LinearOperator.html">LinearOperators</a>.
To compute the NTK alignment, it is necessary to compute the trace of three operators. The denominator operators are positive semi-definite and given by \(\mathcal{(P_0 K_0 P_0^*)^2}, \mathcal{(P_f K_f P_f^*)^2}\). 
The numerator operator measures the inner product between the first and final NTK: \(\mathcal{P_0 K_0 P_0^* P_f K_f K_f^*}\). For the numerator, which may not be PSD, I computed the trace of the operator using <i><a href=https://arxiv.org/abs/2010.09649">Hutch++</a></i>. 
For the denominator terms, which are PSD, I used the <i><a href="https://epubs.siam.org/doi/pdf/10.1137/21M1447623">Nystrom++</a></i> algorithm, which builds on Hutch++ incorporating the PSD assumption. 
</p><p>
Hutch++ builds on the standard Hutchingson-trace estimator by speeding up compute for potentially low-rank operators. I have found that the NTK operator is often low-rank, as in my pre-print above. 
Note that the trace of a square \(n\)-by-\(n\) matrix \(A\) can be expressed as 
$$Tr(A) = \sum_i q_i^T A q_i = Tr(Q^T A Q),$$
for any orthonormal basis \((q_i)_{1\leq i\leq n}\) of \(\mathbb{R}^n\). The idea of Hutch++ is to find a good choice of random \(\hat Q \in \mathbb{R}^{n \times k}\) that accounts for all the variance in \(A\). In other words, we choose \(k\) vectors such that 
$$Tr(A) \approx Tr(\hat Q^T A \hat Q).$$ 
Note \(\hat Q^T A \hat Q\) is a \(k\) by \(k\) matrix, which may be very small. If \(A\) is effectively a low rank matrix (or linear operator), we can thus transform the problem of finding its trace into taking the trace of a small, \(k\) by \(k\) empirically computed matrix. 
</p>


<code style="width:100%;"> <b>Hutch++:</b>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%;"><span></span><span style="color: #080; font-weight: bold">def</span><span style="color: #BBB"> </span><span style="color: #06B; font-weight: bold">trace_hpp</span>(A, nsamp):
  d <span style="color: #333">=</span> A<span style="color: #333">.</span>shape[<span style="color: #00D; font-weight: bold">0</span>]
  S <span style="color: #333">=</span> torch<span style="color: #333">.</span><span style="color: #333"></span>randint(<span style="color: #00D; font-weight: bold">2</span>, size<span style="color: #333">=</span>(d,nsamp))<span style="color: #333">.</span>to(<span style="color: #007020">float</span>) <span style="color: #333">*</span> <span style="color: #00D; font-weight: bold">2</span> <span style="color: #333">-</span> <span style="color: #00D; font-weight: bold">1</span> <span style="color: #888"># Either 1 or -1</span>
  G <span style="color: #333">=</span> torch<span style="color: #333">.</span><span style="color: #333"></span>randint(<span style="color: #00D; font-weight: bold">2</span>, size<span style="color: #333">=</span>(d,nsamp))<span style="color: #333">.</span>to(<span style="color: #007020">float</span>) <span style="color: #333">*</span> <span style="color: #00D; font-weight: bold">2</span> <span style="color: #333">-</span> <span style="color: #00D; font-weight: bold">1</span> <span style="color: #888"># Either 1 or -1</span>
  Q, _ <span style="color: #333">=</span> torch<span style="color: #333">.</span>linalg<span style="color: #333">.</span>qr(A <span style="color: #333">@</span> S)
  B <span style="color: #333">=</span> G <span style="color: #333">-</span> Q <span style="color: #333">@</span> (Q<span style="color: #333">.</span>T <span style="color: #333">@</span> G)
  <span style="color: #080; font-weight: bold">return</span> torch<span style="color: #333">.</span>trace((A <span style="color: #333">@</span> Q)<span style="color: #333">.</span>T <span style="color: #333">@</span> Q) <span style="color: #333">+</span> (<span style="color: #60E; font-weight: bold">1.</span><span style="color: #333">/</span>nsamp) <span style="color: #333">*</span> torch<span style="color: #333">.</span>trace((A <span style="color: #333">@</span> B)<span style="color: #333">.</span>T <span style="color: #333">@</span> B)
</pre></div>
</code>

<h3>Results</h3>

<br>

<p>
Below, I measured the runtime and estimates of the Hutch++ estimator described above. The third panel compares the relative error of each estimate versus the final estimate. It takes about 1 second with 10 samples and 25 seconds with 300 samples.
</p>

<br>


<img src="stats_hubbpp.png"  style="max-width:200%; max-height:200%; width:1200px; height:315px;"/>


<h2 id="cos">Optimizing Cosine Alignment</h2>

<p>
The term I'm really interested in is the NTK cosine alignment, which involves the trace of \(A^T B, A^T A, B^T B\), for operator \(A, B\). 
The biggest bottle-neck by far is the application of the operators, which involves a complex vjp, jvp forward-backward autodiff calculation. 
The naive approach is to compute three traces using Hutch++ above, which does decently well. However, I decided to try design an algorithm that accounts for the redundancy. 
</p>

<p>
Let \(A, B\) by \(n\) by \(n\) and \(S\) be an empirically sampled matrix of shape \(n\) by \(d\) with random orthonormal columns. Compute \(A S, B S\). 
Then, we can estimate the trace of \(A^T A\) by the expect value of the squared norm of the columns of \(A S\):
$$Tr(A^T A) \approx n \mathbb{E}_{i} [\| A s_i\|^2].$$
Likewise we can estimate the trace of \(B^T B\). Finally, 
$$Tr(A^T B) \approx n \mathbb{E}_{i} [\langle B s_i, A s_i\rangle^2].$$
Note that using the consistent <i>matrix skecth</i> \(S\) throughout also correlates the errors and can help reduce the variance of this estimator. Below is the code I used in my package, <a href ="https://github.com/meeree/kpflow">which you can find here.</a> 
It uses pytorch, allowing for possible GPU acceleration (which I haven't tested) and my Operator implementation in that package. 
</p>

<br>

<code style="width:100%;"> <b>Final NTK Alignment Implementation:</b>
<!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%;"><span></span><span style="color: #080; font-weight: bold">def</span><span style="color: #BBB"> </span><span style="color: #06B; font-weight: bold">op_alignment</span>(A, B, nsamp<span style="color: #333">=</span><span style="color: #00D; font-weight: bold">20</span>):
    A_flat <span style="color: #333">=</span> A<span style="color: #333">.</span>flatten() <span style="color: #888"># Flatten input and output shapes. </span>
    B_flat <span style="color: #333">=</span> B<span style="color: #333">.</span>flatten() 
    d <span style="color: #333">=</span> A_flat<span style="color: #333">.</span>shape_in
    G <span style="color: #333">=</span> torch<span style="color: #333">.</span>randn(d, nsamp)
    Q, _ <span style="color: #333">=</span> torch<span style="color: #333">.</span>linalg<span style="color: #333">.</span>qr(G) <span style="color: #888"># Random orthonormal vectors.</span>
    S <span style="color: #333">=</span> (d <span style="color: #333">/</span> nsamp)<span style="color: #333">**</span><span style="color: #60E; font-weight: bold">0.5</span> <span style="color: #333">*</span> Q          
    AS <span style="color: #333">=</span> A_flat<span style="color: #333">.</span>batched_call(S<span style="color: #333">.</span>T)<span style="color: #333">.</span>T
    BS <span style="color: #333">=</span> B_flat<span style="color: #333">.</span>batched_call(S<span style="color: #333">.</span>T)<span style="color: #333">.</span>T
    <span style="color: #080; font-weight: bold">return</span> torch<span style="color: #333">.</span>sum(AS <span style="color: #333">*</span> BS) <span style="color: #333">/</span> (torch<span style="color: #333">.</span>sum(AS <span style="color: #333">*</span> AS) <span style="color: #333">*</span> torch<span style="color: #333">.</span>sum(BS <span style="color: #333">*</span> BS))<span style="color: #333">**</span><span style="color: #60E; font-weight: bold">0.5</span>
</pre></div>
</code>

<br>

<p>
In total, we get about a three times faster estimate with comparable accuracy, as summarized below. 
</p>

<br>

<img src="stats_compare.png"  style="max-width:200%; max-height:200%; width:1200px; height:315px;"/>

<br>

<h2 id="summ">Summary</h2>

<p>
In summary I made an efficient estimator of the NTK alignment metric to compare how the NTK of a model at different points during GD training compare. I found that using Hutch++ was fast and accurate and simple to implement. 
However, I found that, due to redundancy in the cosine similarity expression, we can improve on this by about a factor of three in runtime. 
</p>

<p>
With this code, <u>I found that comparing NTKs with [batches, timesteps, hidden size] = [100, 10, 128], i.e. 128,000 by 128,000 NTK operators, takes about 0.33 seconds with 50 estimator samples, which gives a relative error of below about 1%</u>. 
With such an efficient estimator, it may be possible to practically use this NTK comparison <i>during training</i>, but I'm not sure how or why yet. For now, it allows my collaborators and I to efficiently compare trained models very fast.
</p>

<!-- Navbars -->
<a href="index.html" style="text-decoration: none;">
<div class="top_bar" style="left: 30px;">
goto: main
</div>
</a>

<a href="" style="text-decoration: none;">
<div class="top_bar" style="right: 30px;">
goto: top
</div>
</a>

<div class="top_bar" style="top: 4vw; left: 4vw; bottom: auto; width: 6.5vw; height: 6.5vw; 
    border-radius: 1px; border: 0;  image-rendering: pixelated; border: 1px solid grey;">
	
<label for="toggle" id="toggle-btn"  class="toc-clicker">
<img style="width: 100%; height: 100%;" src="toc.png">
</label>

<input type="checkbox" id="toggle" hidden>

<div id="toc" class="top_bar" style="top: 10.5vw; left: 4vw; bottom: auto; width: 15%; 
height: 50%;  /* ADJUST */
border-radius: 0; border: 0;  image-rendering: pixelated; border: 0; background: none;">

<table class = "toctable" style="width: 100%; height: 100%; padding: 0; background: white; border-radius: 0; margin-top: 10px;">


<tr><td style = "text-align: center; color: black;">
<a href="#top">
Efficient NTK Metrics
</a>
</td></tr>

<tr><td>
<a href="#background">
Background
</a>
</td></tr>

<tr><td>
<a href="#motiv">
Motivation
</a>
</td></tr>

<tr><td>
<a href="#naive">
Naive Approach
</a>
</td></tr>

<tr><td>
<a href="#matrixfree">
Matrix-Free Approaches
</a>
</td></tr>


<tr><td>
<a href="#cos">
Optimizing Cosine Alignment
</a>
</td></tr>

<tr><td>
<a href="#summ">
Summary
</a>
</td></tr>


</table>
</div>
</div>



</body>
</html>
