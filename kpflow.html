<html style="background: url(art/modes_poster.png);
      background-size: repeat; 
      background-size: 50%;">
<head>
<meta name="description" content="James Hazelden">
<link rel=StyleSheet href="nut_styles.css" type="text/css" media=all>
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<link rel="icon" href="./koch-snowflake.png" type="image/x-icon">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body>


<div class="main_text">
<h1>Unpacking K-P Flow:<br><span style = "font-size: 1.4cm; color: #A00;">The Geometry of GD Learning in General Recurrent Models</span></h1>
<hr>

<table style="margin-left: auto; margin-right: auto; width: 80%;font-size:.8cm; border-collapse:separate; border:solid black 3px; border-radius:6px;">

<tr>
<td  colspan=2 style="background-color: white; ">
<b>Links</b>
</td>
</tr>

<tr>
<td style="width:50%; background-color: #eeeeee;">
<a href = "https://arxiv.org/abs/2507.06381">Arxiv Pre-Print</a>
</td>

<td  style="width:50%; background-color: #eeeeee;">
<a href = "https://github.com/meeree/kpflow/">Pytorch Package</a>
</td>
</tr>


</table>


<br>


<p class="center_text">
<b style="font-size: .89cm;">Synopsis</b><br>
This blog post provides a more intuitive exploration of parts our main paper, linked above, which broadly explores the gradient flow of general recurrent models. 
An efficient and in-development package with examples is linked above. See accompanying blog posts on my main page exploring specific aspects of the code. 
</p>
<br>
<br>

<ol style="max-width:45%; font-size: .8cm; text-align:center; text-indent: 0; list-style-type: none;">
	<li> <b>Table of Contents</b>
    <li> <a href="#motiv">Broader Motivation and Prior Work</a>
    <li> <a href="#intro">Introduction</a>
	<li> <a href="#methods">Methods</a>
	<li> <a href="#examples">Examples and Intuition</a>
	<li> <a href="#results">Results</a>
</ol>

<br>
<hr>

<p class = "center_text">
<u>TLDR</u><br>
<i><b>Q: </b>Can we track the evolution of a dynamical system trained by Gradient Descent (GD)?</i><br>
<i><b>A: </b>Yes, but the NTK involves tensor calculus. We need new geometric intuition and dynamical tools.</i>
</p>

<br>
<br>

<img src="schem_gd_flow.png" width=80%/>
<br>


<!--
<p>
Suppose we have some time-varying dynamical system model with tangential dynamics denoted by \(f\), given by a set of vectorized parameters \(\theta\). More specifically, the model takes in some input trajectory, \(x(t)\), for times \(t \in [0, t_{end}]\) and, using this, produces a so-called hidden state trajectory \(h(t)\) by simulating the following initial value problem:
$$\frac{d}{d t} h(t, x) = f(h(t, x), x(t), \theta); \text{ where } h(0) = h_0.$$ 
We may drop the \(x\) above to write simply \(h(t)\) at some points later. Note this is very general: the model may consist of network of neurons, where \(\theta\) are the parameters, or may have a very different form. 
</p>
<p>
In a supervised learning context (e.g. in optimal control or deep learning), we typically want to <i>train</i> this model on a particular dataset of trial inputs \(x \sim X\), so that the network output mimics some target output, \(y^*(t, x)\), on each input-conditioned trial.
</p>

To summarize notation so far:
$$\theta \text{ : Model parameters,}$$
$$x(t) \text{ : Input at time } t \in [0, t_{end}]$$
$$h(t, x) \text{ : Hidden state at time } t \text{ given an input } x.$$

</p>-->

<h1 id="motiv">Broader Motivation and Prior Work</h1>

<br>

<p>
<b>Recurrent Models in Neuroscience and ML</b> Recurrent models in machine learning are a powerful tool that can mimic sequential behavior when fit to data. Practically, this can be used to solve a variety of tasks in deep learning and control. Furthermore, such models can be used as a proxy for understanding how circuity in the brain forms so-called <i>neural manifolds</i>, consisting of low-dimensional dynamical motifs such as fixed points and attractors. In multi-task learning contexts, recurrent models give insights solving complex continual learning or compositional problems.
</p>

<br>

<p>
<b>Many, Many Recurrent Architectures</b> Classically, in control, the most common recurrent model is the Linear Time Invariant (LTI) controller, which is a linear dynamical system that can be trained on a variety of problems and is well analyzed theoretically. In deep learning, the most classical model is the Recurrent Neural Network (RNN), with non-linear time-stepping dynamics. Building on this model are GRUs and LSTMs, which train better, avoiding pitfalls of vanishing and exploding gradients. More recently, there are many diverse recurrent architectures in deep learning, including those with dynamical synapses (MPNs) or State-Space Models (SSMs). Outside of these contexts, we can look to physics and neuroscience to see a very wide range of complex network-based recurrent dynamical systems, such as spin-glasses, Hodgkin-Huxley biophysical neural networks, Hopfield neural networks, energy-based models, and many, many more. 
</p>

<br>

<p>
<b>Training Recurrent Models</b> In control and deep learning, the primary means of "training" such recurrent models is dynamically adapting their parameters (typically weights of the recurrent or output connections) with (potentially stochastic or accelerated) Gradient Descent (GD). More specifically, on discrete models like RNNs or GRUs, we use Backpropagation Through Time (BPTT) to efficiently compute gradients, which are then used to incrementally adjust the parameters. In optimal control or continuous Neural ODEs, this may be instead be labeled as the <i>adjoint method</i>, but it was proven long ago that BPTT and the adjoint method are exactly the same thing. 
</p>

<br>

<p>
<b>Tracking Trained Dynamics</b> In this study, we investigated how the dynamics of a recurrent model trained with GD evolve. More specifically, we define operators that, when composed together, define the <i>hidden-state gradient flow</i> of the a general parameterized recurrent dynamical system. 
</p>

<h1 id="intro">Introduction</h1>

<h3>Gradient Descent (GD) Setup</h3>

<p>
<b>A General Recurrent Model</b> As described, we consider a recurrent dynamical system, trained with GD to approximately mirror target trajectories given a variety of time-varying inputs. We let \(h(t)\) denote the model's hidden state, and \(x(t)\) denote a sample task input. The variable \(t\) denotes the forward pass time and exists in a given time range \([0, t_{end}]\). During the forward-pass inference, an individual task input is chosen, \(x(t)\), from a given distribution of all task inputs; then, the model state \(h(t)\) is simulated from an initial state \(h_0\) to time \(t_{end}\), driven by the input \(x(t)\). The dynamics can be continuously specified or discretely with little change in the mathematics below, so we use assume time is continuous. In this case, the model dynamics take the form:
$$\frac{d}{dt} h(t) = f(h(t), x(t), \theta), \text{ for } t \in (0, t_{end}]; \text { with } h(0) = h_0.$$
Here, \(\theta\) denote the model parameters (e.g., weights and biases) and \(f\) models the tangential dynamics of the hidden state. 
</p>

<p>
<b>Defining the Task</b> For each input \(x(t)\) we associate and desired output target \(y^*(t)\). Furthermore, we define an output function of the hidden state, e.g., the simple affine map:
$$y(t) = W_{out} z(t) + b_{out}.$$
Then, the goal is to minimize a loss function \(\ell(y(t), y^*(t))\) on all possible inputs and at all times. In particular, we define the loss as an average (denoted by \(\langle \cdot \rangle_{x,t}\)):
$$L := \langle \ell(y(t), y^*(t)) \rangle_{x,t}.$$
</p>

<p>
<b>Training the Model with GD</b>
The goal is to minimize the loss \(L\) by tuning the parameters of the model, \(\theta\). To do so, we want to find the best parameters:
$$\theta^* := \arg \min_\theta L.$$
This is typically done by GD, where the parameters are iteratively updated by travelling down the <i>steepest direction in Euclidean parameter space</i>. Specifically, letting \(\delta \theta\) be the perturbation to the parameters at a particular instant in GD, it is given by:
$$\delta \theta := -\eta \nabla_\theta L.$$
Here, \(\eta\) is a learning rate, which I'll just assume is \(1\) throughout the rest of this blog for simplicity. 
</p>

<h3>The Hidden State GD Flow and NTK</h3>

<p>
<b>Tracking the GD Flow</b> We would like to track certain quantities as they evolve under GD. The simplest quantity to track are the parameters, defining the so-called <i>parameter GD flow</i>. However, this quantity is ultimately a proxy for the actual dynamics of the hidden state, which also evolves as GD trains the model. We would like to track how this quantity itself evolves in a meaningful way. The hidden state dynamics can be envisioned as a large vector field, conditioned on the particular input you give the model. 
</p>

<p>
Now, before getting into the details of the objects involved, the general idea (which turns out to work correctly) is simply as follows. Firstly, define the <i>Instantenous Error Signal</i>, Err, by
$$\text{Err} := \nabla_{z} \ell(y, y^*).$$
For example, when we use a squared error loss, \(\ell(y, y^*) := \| y - y^*\|\), the error signal is \(\text{Err} = W_{out}^T (y - y^*)\), a simple residual projected onto the row space of \(W_{out}\). Define a parameter Jacobian \(J_\theta := \nabla_theta h\). Then, by the chain rule, 
$$\nabla_\theta L = J_\theta^T \text{Err}.$$
Given this parameter change, the hidden state itself will approximately change by 
$$\nabla h = -J_\theta \cdot \nabla_\theta L = -J_\theta \cdot J_\theta^T \text{Err}.$$
The outer product \(J_\theta \cdot J_\theta^T\) is referred to as the <i>Neural Tangent Kernel (NTK)</i>.
</p>

<p>
<b>The NTK is a tensor operator</b> In the classical NTK literature, the model considered is a multi-layer perceptron neural network with scalar output, so there is no notion of time, \(t\), and no spatial (hidden unit) dimension. Hence, the NTK is a matrix comparing alignment based on the task inputs, \(x\). In our case, however, the NTK is actually a tensor operator. In a Pytorch implementation, for example, the hidden state will have the form [B, T, H] over batch inputs, \(x\), times, \(t\), and hidden units. The quantity \(J_\theta\) thus has the form [B, T, H, M], where \(M\) is the size of the flattened model parameters. Finally, \(\Theta\) is an linear operator on the space of tensors of the form [B, T, H]. If we discretize it, it will be a <i>massive</i> matrix of shape B*T*H by B*T*H. If indexed in Pytorch, the of \(\Phi\) are given by 
$$\Phi: \mathbb{R}^{B\times T \times H} \rightarrow \mathbb{R}^{B \times T \times H}$$
$$\Phi[b, t, h, b_0, t_0, h_0] = \sum_{m=1}^M J_\theta[b, t, h, m] J_\theta[b_0, t_0, h_0, m].$$
Formally, this is a tensor product on \(J_\theta\) where the parameter dimension is contracted (see my <a href="tensor_calc">blog post going into more depth on tensor calculus</a>). The fact that the NTK is a tensor operator for such general recurrent models has pros and cons. Pros include that it encapsulates a <i>massive</i> amount of information, including insights into GD learning at very granular levels given by eigenfunctions. Indeed, in the next section I'll discuss some of the ways we can <i>reduce the operator</i> to generate different perspectives on the GD learning. However, this is also a since it makes the whole object harder to understand, requiring some tensor math. Another major con is the massive size of this object. Even for very small problems it can be huge (e.g. 100 neurons, batch size 100 and 100 forward-pass times results in \(\Phi\) being 1m by 1m when discretized). Thus, advanced matrix-free methods that do not actually compute the full discretized operator are needed to work with \(\Phi\) (see <a href="trace_estimation">my blog using trace estimation for the NTK</a>, for example).
</p>

<h3>Working with Tensor Operators</h3>

<p>
As mentioned in the prior section, the full empirical NTK for recurrent models is a linear operator on a space of 3-tensors (discretely of shape [B, T, H]). In this section, I'll discuss building some more intuition and practical tools for working with such operators. Indeed, it may be tempting to think that working with such a massive, complex objective is overcomplicating things. However, this object exactly matches Pytorch without any simplication and <i>can be simplified after-the-fact, instead of simplifying at the outset</i>. As we will see later, keeping the full operator in all its complexity allows us to further decompose it into individual components: \(\mathcal{P}\) and \(\mathcal{K}\), the former which intergrates perturbations independently over input trials and the latter which filters perturbations through the parameters.
</p> 
<br>

<p>
<b>Reduced <i>Views</i></b> A simple way to work with this complex operator is to reduce it into simpler "views". The operator itself informs how gradient updates to the hidden state are structured over time, batch inputs and hidden units, which is a ton of information. Sometimes, we would like to know in what subspaces the updates will reside, on average, without consider time or batches. Or, as another example, we want to consider where most of the updates will be concentrated as a signal over time and batches, without thinking about the individual hidden units. 
</p> <p>
To generate such views, we define a method of reducing the operator over particular axes. Given an axis (time, batches or hidden units), the view of the tensor essentially performs streaming over that axis. For example, let's define the time-averaged NTK \(\langle \Phi \rangle_t\). This is a linear operator now on 2-tensors of shape [B, H]. Specifically, 
$$\langle \Phi \rangle_t[b, h, b_0, h_0] = \langle \Phi[b, h, t, b_0, h_0, t_0] \rangle_{t, t_0},$$
i.e. we average over the time axes. This operator takes in tensors of shape [B, H], produces a tensor of shape [B, T, H] by making T copies, applies \(\Phi\) to this tensor, then averages the final result over the time axis. In my code, you can find the implementation in <b>op_common.py</b>, called <b>AveragedOperator</b>. Similarly, we can define averaging operations over any of the axes: time, hidden units, or batches, or multiple simultaneously. 
</p> 
<br>
<p class="center_text"><b>Some Example Use Cases</b></p>
<p>
<b>As a Matrix Over Hidden Units</b> Using these views, we can measure interpretable properties of the NTK and gradient flow. For example, suppose want to quantify where the gradient updates will be concentrated in the hidden space, irregardless of time or batch trials. Then, we can measure the averaged operator \(\langle \Phi \rangle_{x,t}\), which is now an H by H matrix. We can then measure the <u>singular vectors</u> \(\{v_i\}_{i=1...n}\) of this matrix, and its <u>singular values</u> \(\{\sigma_i\}_{i=1...n}\). The principle vector \(v_1\) explains exactly which types of hidden space inputs will maximally stimulate the operator \(\Phi\), on average over input batches and times. Note that two trials with different inputs, \(x_1, x_2\) to the hidden state, may produce trajectories in completely disparate regions of the hidden space. However, by design, both of these directions will factor in distinctly to the averaged SVD (see my paper Appendix for more explanation of this). If we want to see where gradient updates will be correlated, we can use the <u>left singular vectors</u>, \(\{u_i\}_{i=1...n}\): the first vector \(u_1\) explains where \(\Phi\)'s updates will be contentrated if we given it tons of random input signals, i.e. where GD is likely to concentrate. Finally, the <u>effective rank</u> of the operator \(\langle \Phi \rangle_{x,t}\) explains how hidden space GD updates are constrained: if it is low, it means the operator will be constrained to producing updates in a small, low-dimensional hidden subspace. 
</p>
<p>
<b>Eigenfunction Signals Averaging Hidden Space</b> As another example, we can consider the operator \(\langle \Phi \rangle_{h}\), averaged over spatial hidden units. If we take the SVD of this operator, we get <u>input-dependent eigenfunctions</u> \(\{\phi_{b,t}\}_{b=1...B,t=1...T}\). Each eigenfunction explains which points in time are most crucial to stimulating \(\Phi\), for each individual one of the batch trials. On trial 1, it might say that a particular time window of the task is very crucial, i.e. most of the learning will be concentrated there, while on trial 2, with a different batch input for the model, there could be a completely distinct time window. Consequently, <i>these eigenfunction signals explain the temporal structure imposed by the task on learning</i>, accross each input-driven trial of the model inference. Since the operator is averaged over hidden units, we don't take into account <i>which</i> hidden unit to stimulate, instead we care about which times or indendent input trials are most relevant to the GD update.
</p>

<p>Below is a figure more clearly explained in my paper, illustrating the eigenfunctions of the operator \(\langle \mathcal{K} \rangle_{h}\), averaged over hidden units, as in the previous paragraph. A full description of \(\mathcal{K}\) itself is detailed below. The color of each individual trajectory indicate distinct batches. Note that each eigenfunction is a tensor of shape [B, T], as above, informing, on each batch trial, which times are most significant for stimulating the operator. For task setup and more comprehensive details, see the paper itself.
</p>

<br>

<br>
<img src="eigfuns_k.png" width=90% style="border-color: black; border-width: 10px;">

<br>

<br>
<br>

<p class="center_text"><b>The Full Operator SVD</b></p>
<p>
Simular to the previous use case, we can take the SVD of the full operator, \(\Phi\), without averaging any axes. Then, the eigenfunctions and singular values you get inform how to stimulate \(\Phi\) individually at a very granular level: at each timestep of forward evaluation of the model, hidden unit, and on each distinct input-driven trial.
</p>

<h3>Decomposing the NTK into \(\mathcal{P}\) and \(\mathcal{K}\)</h3>

<p>
To summarize the work so far, I have discussed the NTK, expained why for recurrent dynamical models it is a linear operator on 3-tensors and finally I've discussed how to work with such a weird object. In this section, I will introduce the decomposition in my main paper, breaking the recurrent NTK for these general models into a product:
$$\Phi = \mathcal{P} \mathcal{K} \mathcal{P}^*$$
where \(\mathcal{P, K}\) are linear operators themselves, which I'll now define, and \(\mathcal{P}^*\) denotes the <i>Hermitian adjoint</i> of the operator \(\mathcal{P}\) (typically just the transpose of the discretized matrix). 
</p>


<p>
I gave the operators the following names: 
<br>
<br>
<ol style="max-width:45%; font-size: .8cm; text-align:center; text-indent: 0;">
    <li> \(\mathcal{P}\) the <i>Propagation Operator</i> which integrates perturbations to the hidden state dynamics forwards over time. This is exactly the Green's function of the model dynamics!
        <br>
        <br>
    <li> \(\mathcal{K}\) the <i>Parameter Operator</i> which filters tangential perturbations to the model instantaneous dynamics (the term \(f\) in dynamics) through the parameters, essentially producing a best estimate of how to achieve those changes through the model weights and biases.
</ol>
</p>

<p>
</p>

<div class="top_bar" style="left: 30px;">
<a href="index.html" style="text-decoration: none;">goto: main</a>
</div>

<div class="top_bar" style="right: 30px;">
<a href="" style="text-decoration: none;">goto: top</a>
</div>

</body>
</html>
