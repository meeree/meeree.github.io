<html style="background: url(art/modes_poster.png);
      background-size: repeat; 
      background-size: 50%;
>
<head>
<meta name="description" content="James Hazelden">
<link rel=StyleSheet href="nut_styles.css" type="text/css" media=all>
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<link rel="icon" href="./koch-snowflake.png" type="image/x-icon">
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body style="">
<div class="main_text">
<h1 id="top">Unpacking <i>K-P</i> Flow:<br><span style = "font-size: 1.4cm; color: #555;">The Geometry of GD Learning in General Recurrent Models</span></h1>
<hr>

<table style="margin-left: auto; margin-right: auto; width: 80%;font-size:.8cm; border-collapse:separate; border:solid black 3px; border-radius:6px;">

<tr>
<td  colspan=2 style="background-color: white; ">
<b>Relevant Links</b>
</td>
</tr>

<tr>
<td style="width:50%; background-color: #eeeeee;">
<a href = "https://arxiv.org/abs/2507.06381">Arxiv Pre-Print</a>
</td>

<td  style="width:50%; background-color: #eeeeee;">
<a href = "https://github.com/meeree/kpflow/">Pytorch Package</a>
</td>
</tr>


</table>

<!--<ol style="max-width:45%; font-size: .8cm; text-align:center; text-indent: 0; list-style-type: none;">
	<li> <b>Table of Contents</b>
    <li> <a href="#motiv">Broader Motivation and Prior Work</a>
    <li> <a href="#intro">Introduction</a>
	<li> <a href="#methods">Methods</a>
	<li> <a href="#examples">Examples and Intuition</a>
	<li> <a href="#results">Results</a>
</ol>-->


<br>

<p class = "center_text">
<u>TLDR</u><br>
<i><b>Q: </b>Can we track the evolution of a dynamical system trained by Gradient Descent (GD)?</i><br>
<i><b>A: </b>Yes, but the NTK involves tensor calculus. We need new geometric intuition and dynamical tools.</i>
</p>

<br>
<br>



<p class="center_text">
<b style="font-size: .89cm;">Synopsis</b><br>
This blog post provides a more intuitive exploration of some parts of our main paper, linked above, which broadly explores the gradient flow of general recurrent models. 
An efficient and in-development package with examples is linked above. See accompanying blog posts on my main page exploring specific aspects of the code. 
</p>
<br>
<br>

<br>
<hr>

<h1 id="motiv">Broader Motivation and Prior Work</h1>

<br>

<p>
<b>Recurrent Models in Neuroscience and ML</b> Recurrent models in machine learning are a powerful tool that can mimic sequential behavior when fit to data. Practically, this can be used to solve a variety of tasks in deep learning and control. Furthermore, such models can be used as a proxy for understanding how circuity in the brain forms so-called <i>neural manifolds</i>, consisting of low-dimensional dynamical motifs such as fixed points and attractors. In multi-task learning contexts, recurrent models give insights solving complex continual learning or compositional problems.
</p>

<br>

<p>
<b>Many, Many Recurrent Architectures</b> Classically, in control, the most common recurrent model is the Linear Time Invariant (LTI) controller, which is a linear dynamical system that can be trained on a variety of problems and is well analyzed theoretically. In deep learning, the most classical model is the Recurrent Neural Network (RNN), with non-linear time-stepping dynamics. Building on this model are GRUs and LSTMs, which train better, avoiding pitfalls of vanishing and exploding gradients. More recently, there are many diverse recurrent architectures in deep learning, including those with dynamical synapses (MPNs) or State-Space Models (SSMs). Outside of these contexts, we can look to physics and neuroscience to see a very wide range of complex network-based recurrent dynamical systems, such as spin-glasses, Hodgkin-Huxley biophysical neural networks, Hopfield neural networks, energy-based models, and many, many more. 
</p>

<br>

<p>
<b>Training Recurrent Models</b> In control and deep learning, the primary means of "training" such recurrent models is dynamically adapting their parameters (typically weights of the recurrent or output connections) with (potentially stochastic or accelerated) Gradient Descent (GD). More specifically, on discrete models like RNNs or GRUs, we use Backpropagation Through Time (BPTT) to efficiently compute gradients, which are then used to incrementally adjust the parameters. In optimal control or continuous Neural ODEs, this may be instead be labeled as the <i>adjoint method</i>, but it was proven long ago that BPTT and the adjoint method are exactly the same thing. 
</p>

<br>

<p>
<b>Tracking Trained Dynamics</b> In this study, we investigated how the dynamics of a recurrent model trained with GD evolve. More specifically, we define operators that, when composed together, define the <i>hidden-state gradient flow</i> of the a general parameterized recurrent dynamical system. 
</p>

<h1 id="intro">Introduction</h1>

<h3>Gradient Descent (GD) Setup</h3>

<p>
<b>A General Recurrent Model</b> As described, we consider a recurrent dynamical system, trained with GD to approximately mirror target trajectories given a variety of time-varying inputs. We let \(h(t)\) denote the model's hidden state, and \(x(t)\) denote a sample task input. The variable \(t\) denotes the forward pass time and exists in a given time range \([0, t_{end}]\). During the forward-pass inference, an individual task input is chosen, \(x(t)\), from a given distribution of all task inputs; then, the model state \(h(t)\) is simulated from an initial state \(h_0\) to time \(t_{end}\), driven by the input \(x(t)\). The dynamics can be continuously specified or discretely with little change in the mathematics below, so we use assume time is continuous. In this case, the model dynamics take the form:
$$\frac{d}{dt} h(t) = f(h(t), x(t), \theta), \text{ for } t \in (0, t_{end}]; \text { with } h(0) = h_0.$$
Here, \(\theta\) denote the model parameters (e.g., weights and biases) and \(f\) models the tangential dynamics of the hidden state. 
</p>

<p>
<b>Defining the Task</b> For each input \(x(t)\) we associate and desired output target \(y^*(t)\). Furthermore, we define an output function of the hidden state, e.g., the simple affine map:
$$y(t) = W_{out} h(t) + b_{out}.$$
Then, the goal is to minimize a loss function \(\ell(y(t), y^*(t))\) on all possible inputs and at all times. In particular, we define the loss as an average (denoted by \(\langle \cdot \rangle_{x,t}\)):
$$L := \langle \ell(y(t), y^*(t)) \rangle_{x,t}.$$
</p>

<p>
<b>Training the Model with GD</b>
The goal is to minimize the loss \(L\) by tuning the parameters of the model, \(\theta\). To do so, we want to find the best parameters:
$$\theta^* := \arg \min_\theta L.$$
This is typically done by GD, where the parameters are iteratively updated by travelling down the <i>steepest direction in Euclidean parameter space</i>. Specifically, letting \(\delta \theta\) be the perturbation to the parameters at a particular instant in GD, it is given by:
$$\delta \theta := -\eta \nabla_\theta L.$$
Here, \(\eta\) is a learning rate, which I'll just assume is \(1\) throughout the rest of this blog for simplicity. 
</p>

<h3 id = "ntkflow">The Hidden State GD Flow and NTK</h3>

<p>
<b>Tracking the GD Flow</b> We would like to track certain quantities as they evolve under GD. The simplest quantity to track are the parameters, defining the so-called <i>parameter GD flow</i>. However, this quantity is ultimately a proxy for the actual dynamics of the hidden state, which also evolves as GD trains the model. We would like to track how this quantity itself evolves in a meaningful way. The hidden state dynamics can be envisioned as a large vector field, conditioned on the particular input you give the model. 
</p>

<p>
Now, before getting into the details of the objects involved, the general idea (which turns out to work correctly) is simply as follows. Firstly, define the <i>Instantenous Error Signal</i>, Err, by
$$\text{Err} := \nabla_{h} \ell(y, y^*).$$
For example, when we use a squared error loss, \(\ell(y, y^*) := \| y - y^*\|\), the error signal is \(\text{Err} = W_{out}^T (y - y^*)\), a simple residual projected onto the row space of \(W_{out}\). Then, by the chain rule, 
$$\nabla_\theta L = (\frac{d h}{d \theta})^T \text{Err}.$$
Given this small parameter change, the hidden state itself will approximately change linearly by 
$$\delta h = -\frac{d h}{d \theta} \cdot \nabla_\theta L = -\frac{d h}{d \theta} \cdot \frac{d h}{d \theta}^T \text{Err}.$$
In this blog, I will refer to the Jacobian outer product \(\Phi := \frac{d h}{d \theta} \cdot \frac{d h}{d \theta}^T\) as the <b><i>Neural Tangent Operator (NTO)</i></b>.
</p>

<hr>

<h3>The Neural Tangent Operator</h3>
<br>

<figure>
<a href="nto.png"><img src="nto.png" width=70%/></a>
<figcaption><p><b>Fig 1</b> The NTO and NTK for a scalar-output multi-layer-perceptron neural network. The classical NTK \(\Theta\) is a matrix quantifying how GD changes align over batch inputs. By contrast, the NTO \(\Phi\) measures how the activations at every layer change. </p></figcaption>
</figure>

<p>
I use the non-standard term "neural tangent operator" (NTO) to distinguish this from the classical Neural Tangent Kernel (NTK). 
The classical NTK is a matrix tracking how the output of a scalar-output multi-layer-perceptron neural network evolves as we train the weights and biases. 
This NTK is a matrix of shape [B, B], where B is the number of batch inputs on which we evaluate the model and adjust it. 
In contrast, the NTO for this model would track how <i>all hidden activations</i>, \(h(t)\) for each layer \(t\), evolve over gradient descent. 
It would be a tensor of shape [T, B, B, T], or, if we vectorize, a matrix of shape [B*T, B*T], so it is much larger. 
We could also refer to the NTO as the "full NTK," "extended NTK," or "hidden state NTK" I just chose Neural Tangent Operator to make it clear that it is much larger and typically operates on multi-dimensional things, not just vectors. 
Below I detail this more.  
</p>

<p>
<b>The Classical NTK</b> In the classical NTK literature, the model considered is a multi-layer perceptron neural network with scalar output, so there is no notion of time, \(t\), and no spatial (hidden unit) dimension. 
Hence, the NTK is just a B by B matrix over all batch input trials. 
The NTK entries pinpoint how much a GD corrections correlate over batch trials: do corrections to the parameters proposed by two distinct task inputs agree, not overlap at all, or disagree, leading to loss when we do the actual parameter update. 
I will use the notation \(\Theta\) to denote the NTK.
Concretely, if \(y = w^T h\) where \(w\) is a vector, then we can relate the NTO, \(\Phi\), and NTK, \(\Theta\):
$$\Theta = w^T \Phi w,$$
i.e. the NTO is the actually complicated part of the whole operator. 
</p> 
<p>
<b>The NTO is a tensor operator</b> In our case, however, the NTO is actually a tensor operator. In a Pytorch implementation, for example, the hidden state will have the form [B, T, H] over batch inputs, \(x\), times, \(t\), and hidden units. The quantity \(\frac{d h}{d \theta}\) thus has the form [B, T, H, M], where \(M\) is the size of the flattened model parameters. Finally, \(\Theta\) is an linear operator on the space of tensors of the form [B, T, H]. If we discretize it, it will be a <i>massive</i> matrix of shape B*T*H by B*T*H. If indexed in Pytorch, the of \(\Phi\) are given by 
$$\Phi: \mathbb{R}^{B\times T \times H} \rightarrow \mathbb{R}^{B \times T \times H}$$
$$\Phi[b, t, h, b_0, t_0, h_0] = \sum_{m=1}^M \frac{d h}{d \theta}[b, t, h, m] \frac{d h}{d \theta}[b_0, t_0, h_0, m].$$
Formally, this is a tensor product on \(\frac{d h}{d \theta}\) where the parameter dimension is contracted (see my <a href="tensor_calc">blog post going into more depth on tensor calculus</a>). The fact that the NTO is a tensor operator for such general recurrent models has pros and cons. Pros include that it encapsulates a <i>massive</i> amount of information, including insights into GD learning at very granular levels given by eigenfunctions. Indeed, in the next section I'll discuss some of the ways we can <i>reduce the operator</i> to generate different perspectives on the GD learning. However, this is also a since it makes the whole object harder to understand, requiring some tensor math. Another major con is the massive size of this object. Even for very small problems it can be huge (e.g. 100 neurons, batch size 100 and 100 forward-pass times results in \(\Phi\) being 1m by 1m when discretized). Thus, advanced matrix-free methods that do not actually compute the full discretized operator are needed to work with \(\Phi\) (see <a href="trace_estimation">my blog using trace estimation for the NTO</a>, for example).
</p>

<h3 id="working">Working with Tensor Operators</h3>

<p>
As mentioned in the prior section, the full empirical NTO for recurrent models is a linear operator on a space of 3-tensors (discretely of shape [B, T, H]). In this section, I'll discuss building some more intuition and practical tools for working with such operators. Indeed, it may be tempting to think that working with such a massive, complex objective is overcomplicating things. However, this object exactly matches Pytorch without any simplication and <i>can be simplified after-the-fact, instead of simplifying at the outset</i>. As we will see later, keeping the full operator in all its complexity allows us to further decompose it into individual components: \(\mathcal{P}\) and \(\mathcal{K}\) which are operators that have their own distinct structure that can be exploited.
</p> 
<br>


<figure>
<a href="schematic_reduction.png"><img src="schematic_reduction.png" width=100%/></a>
<figcaption><p><b>Fig 1</b> Schematic of heirarchy of <i>views</i> attainable by different reductions of the full hidden-state NTO operator (or related operators). </p></figcaption>
</figure>

<p>
<b>Reduced <i>Views</i></b> A simple way to work with this complex operator is to reduce it into simpler "views". The operator itself informs how gradient updates to the hidden state are structured over time, batch inputs and hidden units, which is a ton of information. Sometimes, we would like to know in what subspaces the updates will reside, on average, without consider time or batches. Or, as another example, we want to consider where most of the updates will be concentrated as a signal over time and batches, without thinking about the individual hidden units. 
</p> <p>
To generate such views, we define a method of reducing the operator over particular axes. Given an axis (time, batches or hidden units), the view of the tensor essentially performs streaming over that axis. For example, let's define the time-averaged NTO \(\langle \Phi \rangle_t\). This is a linear operator now on 2-tensors of shape [B, H]. Specifically, 
$$\langle \Phi \rangle_t[b, h, b_0, h_0] = \langle \Phi[b, h, t, b_0, h_0, t_0] \rangle_{t, t_0},$$
i.e. we average over the time axes. This operator takes in tensors of shape [B, H], produces a tensor of shape [B, T, H] by making T copies, applies \(\Phi\) to this tensor, then averages the final result over the time axis. In my code, you can find the implementation in <b>op_common.py</b>, called <b>AveragedOperator</b>. Similarly, we can define averaging operations over any of the axes: time, hidden units, or batches, or multiple simultaneously. 
</p> 
<br>
<p class="center_text"><b>Some Example Use Cases</b></p>

<figure>
<a href="eigfuns_k.png"><img src="eigfuns_k.png" width=90%/></a>
<figcaption><p><b>Fig 2</b> Eigenfunctions and effective rank of the parameter operator \(\langle \mathcal{K} \rangle_{h}\), reduced over the hidden dimension. </p></figcaption>
</figure>

<p>
<b>As a Matrix Over Hidden Units</b> Using these views, we can measure interpretable properties of the NTO and gradient flow. For example, suppose want to quantify where the gradient updates will be concentrated in the hidden space, irregardless of time or batch trials. Then, we can measure the averaged operator \(\langle \Phi \rangle_{x,t}\), which is now an H by H matrix. We can then measure the <u>singular vectors</u> \(\{v_i\}_{i=1...n}\) of this matrix, and its <u>singular values</u> \(\{\sigma_i\}_{i=1...n}\). The principle vector \(v_1\) explains exactly which types of hidden space inputs will maximally stimulate the operator \(\Phi\), on average over input batches and times. Note that two trials with different inputs, \(x_1, x_2\) to the hidden state, may produce trajectories in completely disparate regions of the hidden space. However, by design, both of these directions will factor in distinctly to the averaged SVD (see my paper Appendix for more explanation of this). If we want to see where gradient updates will be correlated, we can use the <u>left singular vectors</u>, \(\{u_i\}_{i=1...n}\): the first vector \(u_1\) explains where \(\Phi\)'s updates will be contentrated if we given it tons of random input signals, i.e. where GD is likely to concentrate. Finally, the <u>effective rank</u> of the operator \(\langle \Phi \rangle_{x,t}\) explains how hidden space GD updates are constrained: if it is low, it means the operator will be constrained to producing updates in a small, low-dimensional hidden subspace. 
</p>

<p>
<b>Eigenfunction Signals Averaging Hidden Space</b> As another example, we can consider the operator \(\langle \Phi \rangle_{h}\), averaged over spatial hidden units. If we take the SVD of this operator, we get <u>input-dependent eigenfunctions</u> \(\{\phi_{b,t}\}_{b=1...B,t=1...T}\). Each eigenfunction explains which points in time are most crucial to stimulating \(\Phi\), for each individual one of the batch trials. On trial 1, it might say that a particular time window of the task is very crucial, i.e. most of the learning will be concentrated there, while on trial 2, with a different batch input for the model, there could be a completely distinct time window. Consequently, <i>these eigenfunction signals explain the temporal structure imposed by the task on learning</i>, accross each input-driven trial of the model inference. Since the operator is averaged over hidden units, we don't take into account <i>which</i> hidden unit to stimulate, instead we care about which times or indendent input trials are most relevant to the GD update.
</p>

<p>Below is a figure more clearly explained in my paper, illustrating the eigenfunctions of the operator \(\langle \mathcal{K} \rangle_{h}\), averaged over hidden units, as in the previous paragraph. A full description of \(\mathcal{K}\) itself is detailed below. The color of each individual trajectory indicate distinct batches. Note that each eigenfunction is a tensor of shape [B, T], as above, informing, on each batch trial, which times are most significant for stimulating the operator. For task setup and more comprehensive details, see the paper itself.
</p>
<!--
<h2>Formalizing Reduced Operators</h2>

<p>
Consider a [2, 5, 5, 2] tensor \(W\), which we write as a block matrix of the form
$$W = \begin{pmatrix} W_{00} & W_{01} \\ W_{10} & W_{11} \end{pmatrix}$$
where each block \(W_{ij}\) is 5 by 5. Let's say we wanted to take the SVD of this operator "reduced over the 2 axes." 
Explictly, we can basically write the operator as a 5 by 20 wide matrix of the form
$$W_{flat} = \begin{pmatrix} W_{00} & W_{01} & W_{10} & W_{11} \end{pmatrix}$$ 
Using this, we can measure a covariance averaging over the 2 axes by forming 
$$G = \frac{W_{flat} W_{flat}^*}{2 * 2} = \langle W_{ij} W_{ij}^* \rangle_{i, j}.$$
From \(G\), we then take the eigendecomposition, \(G = U \Sigma^2 U^T\). The \(U\) specifies singular vectors and \(\Sigma\) reduced singular values.
</p>

<p>
We'll formalize this now. For an operator \(W\) of shape [T, T, ....] (or re-organized), we define the \(t\)-mean view of \(W\) as 
$$\langle W \rangle_t := \frac{\text{tr}_t(W)}{t} = \frac{\sum_t W_{t, t}}{T}.$$
Note this averages over time and can lose information. However, in PCA or SVD, we take the Grammian operator \(W W^*\) before averaging, equivalent to a second moment (covariance) not first moment (mean). 
In particular to compute the \(t\)-reduced SVD with left singular vectors, we have the following three steps:
</p>
<br>
<ol>
<center>\(t\)-Reduced SVD:</center>
<li> Define \(G = W W^* / T\)
<li> Estimate or calculate \(\langle G \rangle_t := \frac{\sum_t G_{t,t}}{T} = \frac{\text{tr}_t(G)}{T}\)
<li> Eigendecompose \(\langle G \rangle_t = U \Sigma^2 U^*\)
</ol>
<br>
<br>

<p>
This algorithm looks nice, but in reality we <b>do not want to explictly construct anything</b> here, instead estimating, since everything can be huge!

</p> 
 
 Then,
$$\frac{W W^*}{2} = \begin{pmatrix} \langle W_{0t} W_{0t}^*\rangle_t & W_{0t} W_{1t}^*\rangle_t \\ 
</p>

-->

<p class="center_text"><b>The Full Operator SVD</b></p>
<p>
Simular to the previous use case, we can take the SVD of the full operator, \(\Phi\), without averaging any axes. Then, the eigenfunctions and singular values you get inform how to stimulate \(\Phi\) individually at a very granular level: at each timestep of forward evaluation of the model, hidden unit, and on each distinct input-driven trial.
</p>
<br>
<hr>

<h3 id="kpflow">Decomposing the NTO into \(\mathcal{P}\) and \(\mathcal{K}\)</h3>

<figure>
<a href="schem_gd_flow.png"><img src="schem_gd_flow.png" width=80%/></a>
<figcaption><p><b>Fig 3</b> Schematic of the K-P flow decomposition of the full NTO associated with a recurrent dynamical system, as detailed in this blog post. </p></figcaption>
</figure>

<p>
To summarize the work so far, I have (1) defined the NTO, (2) expained why for recurrent dynamical models it is a linear operator on 3-tensors and (3) discussed how to work with such a weird object. 
In this section, I will introduce the <b><i>KP</i>-flow decomposition</b> in my main paper, breaking the recurrent NTO for these general models into a product:
$$\Phi = \mathcal{P} \mathcal{K} \mathcal{P}^*$$
where \(\mathcal{P, K}\) are linear operators themselves, which I'll now define, and \(\mathcal{P}^*\) denotes the <i>Hermitian adjoint</i> of the operator \(\mathcal{P}\) (typically just the transpose of the discretized matrix). 
</p>

<p>
</p>


<figure>
<a href="pert_schematic.png"><img src="pert_schematic.png" width=70%/></a>
<figcaption><p><b>Fig 4</b> Hidden state trajectories conditioned on the task inputs \(x_j\). The model dynamics \(f\) specify the tangential velocity of each trajectory. Perturbing \(h\) is given as a sequence of perturbations: first the parameters, \(\delta \theta\), then the tangents, \(\delta f\) and finally the state, \(\delta h\).</p></figcaption>
</figure>

<p>
Consider the model state on task input \(x_j\), which is given by simulating the model from time \(0\) to any \(t\),
$$h_j(t) = \int_0^t f(h_j(t_0), x_j(t_0), \theta) \text{d} t_0.$$
Then, there is a chain of perturbations leading to the change \(h_j(t)\) to the hidden state on trial \(j\) at time \(t\). 
Firstly, there is a perturbation, \(\delta \theta\), to the parameters according to the GD step. 
Next, this gives rise to a perturbation, \(\delta f\), to the tangential dynamics at <i>every</i> time and trial. 
When there is a discrete number of timesteps T, H hidden units and B task inputs, \(\delta f\) is a tensor of shape [B, T, H]. 
Finally, these <i>local changes</i> \(\delta f\) are integrated over time to give rise to the true hidden state change.  
</p>

<p>
In total, there are two constraints on learning: (1) filtering through parameters, and (2) local changes. (1) means that dynamical changes can only be attained by perturbing \(\theta\). 
Internally, there is a mean yielding this perturbation, which can lead to misdirection or zeroing of gradient signals.
For example, if we have a dynamical system with a single parameter, the desired dynamical changes may be very complex, but clearly perturbing a single parameter will not be able to express a very wide range of such perturbations. 
(2) comes from the temporal dependence between the hidden state \(h_j(t)\) and all prior hidden states, \(h_j(s)\). 
Perturbations \(\delta h_j\) manifest specifically by first perturbing the tangents, \(\delta f\), then accumulating these. 
</p>

<p>
The two constraints described correspond perfectly to the operators \(\mathcal{K}\) and \(\mathcal{P}\). In particular, \(\mathcal{K}\) filters dynamical perturbations through parameters of the model, constraining the range of such perturbations. 
Furthermore, \(\mathcal{P}\) integrates tangential changes, \(\delta f\), accumulating them into \(\delta h\). Both are linear operators we formally define below.
</p>

<p>
In total, the gradient flow operator \(\Phi\) can be decomposed in terms of \(\mathcal{P,K}\) as 
$$\Phi = \mathcal{P K P^*},$$
where \(\mathcal{P^*}\) is the <i>Hermitian adjoint</i> operator corresponding to \(\Phi\). 
</p>

<figure>
<a href="poster_neuroai_final.png"><img src="poster_neuroai_final.png" width=70%/></a>
<figcaption><p><b>Fig 5</b> Poster.</p></figcaption>
</figure>

<!--

<p>
<b>Formal Derivation</b> Let's take a step back and consider how the backpropgation forms gradients in recurrent models. 
This process is known as Backpropagation-Through-Time (BPTT) in discrete contexts and is provably identical to the Adjoint Method in continuous contexts. 
</p>
<p>
<b>Computing the Adjoint (the \(\mathcal{P}^*\) operator):</b> With the error signal, \(\text{Err} := \nabla_h \ell(y, y^*)\), as above, we backpropagate to accumulate the error signal over time in reverse time, computing the so-called <i>adjoint</i> defined by 
$$a := \nabla_h L = \nabla_h \langle \ell \rangle_t.$$
In particular, the backpropagation stage computes the ajdoints backwards through time as:
$$a(t) = J_h(t)^T a(t+1) + \text{Err}(t),$$
where \(J_h(t) = d h(t+1) / d h(t) = d f / d h(t)\) is the state-Jacobian of the model dynamics. 
In my formulation, this is exactly what the operator \(\mathcal{P}^*\) does, describing the transformation from the error signal, \(\text{Err}\), to the adjoint, \(a\), given exactly by backpropagating, as above. 
Essentially, the operator \(\mathcal{P}\) accumulates perturbations to the hidden state forwards through time using \(J_h\)(, while its adjoint, \(\mathcal{P}^*\), backpropagates using \(J_h^T\), as above. 
Thus, we can think of \(\mathcal{P}\) as a <i>Forward Propagation Operator</i> while its adjoint \(\mathcal{P}^*\) is a <i>Backward Propagation Operator</i>.
This is described in more detail below. 
<p>

<p>
<b>Computing \(\delta \theta\) (the \(\mathcal{K}\) operator):</b> Once the adjoint is computed, the parameter perturbation GD will propose is given by
$$\delta \theta = -\langle J_\theta^T \cdot a \rangle_{x, t},$$
where \(J_\theta^T\) is the one-step Jacobian \(d f / d \theta\). This defines the \(\mathcal{K}\) operator. In particular, \(\mathcal{K}\) maps a particular tensor \(q\) of shape [B, T, H] to a new tensor of the same shape by 
$$(\mathcal{K} q)[b, t, i] = J_\theta[b, t, i] \cdot \langle J_\theta^T \cdot a \rangle_{x,t}.$$
</p>

<p>
In total, \(\mathcal{K} a = \mathcal{K} \mathcal{P}^* \text{Err}\) defines the one-step perturbations, \(\delta f\) to the model proposed by GD, before integrating these up over time. 
Integrating them over time transforms them into the actual changes \(\delta h\) to the hidden state and is exactly explained by the forward operator \(\mathcal{P}\). 
In particular, the full GD flow is 
$$\delta h = - \mathcal{P K P^*} (\text{Err}),$$ 
decomposing the NTO \(\Phi = \mathcal{P K P^*}\).
</p>

<p>
WHY THE ORGANIZATION MATTERS. ALSO, GIVE P VS K RANDOM INPUTS.
</p>-->

<a href="index.html" style="text-decoration: none;">
<div class="top_bar" style="left: 30px;">
goto: main
</div>
</a>

<a href="" style="text-decoration: none;">
<div class="top_bar" style="right: 30px;">
goto: top
</div>
</a>

<div class="top_bar" style="top: 4vw; left: 4vw; bottom: auto; width: 6.5vw; height: 6.5vw; 
    border-radius: 1px; border: 0;  image-rendering: pixelated; border: 1px solid grey;">
	
<label for="toggle" id="toggle-btn"  class="toc-clicker">
<img style="width: 100%; height: 100%;" src="toc.png">
</label>

<input type="checkbox" id="toggle" hidden>

<div id="toc" class="top_bar" style="top: 10.5vw; left: 4vw; bottom: auto; width: 15%; 
height: 60%;  /* ADJUST */
border-radius: 0; border: 0;  image-rendering: pixelated; border: 0; background: none;">

<table class = "toctable" style="width: 100%; height: 100%; padding: 0; background: white; border-radius: 0; margin-top: 10px;">


<tr><td style = "text-align: center; color: black;">
<a href="#top">
Unpacking K-P Flow
</a>
</td></tr>

<tr><td>
<a href="#motiv">
Broader Motivation and Prior Work
</a>
</td></tr>

<tr><td>
<a href="#intro">
Introduction
</a>
</td></tr>

<tr><td>
<a href="#ntkflow">
The Hidden State GD Flow and NTK
</a>
</td></tr>

<tr><td>
<a href="#working">
Working with Tensor Operators
</a>
</td></tr>


<tr><td>
<a href="#kpflow">
Decomposing the NTK into \(\mathcal{P}\) and \(\mathcal{K}\) 
</a>
</td></tr>


</table>
</div>
</div>



</body>
</html>
