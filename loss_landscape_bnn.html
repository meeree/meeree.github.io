<html>
<head>
<meta name="description" content="James Hazelden">
<link rel=StyleSheet href="nut_styles.css" type="text/css" media=all>
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<link rel="icon" href="./koch-snowflake.png" type="image/x-icon">
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body>

<div class="main_text">
<h1>Loss Landscapes Emerging from BNNs</h1>
<hr>

<ol style="max-width:45%;">
    <li> <a href="#intro">Introduction</a> 
    <li> <a href="#simple">Most Simple Case</a> 
    <ol>
        <li> <a href="#loss_simple">Loss Landscape</a></li> 
        <li> <a href="#gradients_simple">Gradient Descent on This Landscape</a></li> 
    </ol>
    <li> <a href="#molification">Molification</a> 
    <ol>
        <li> <a href="#molify_loss">Molifying Loss Curve</a></li> 
        <li> <a href="#molify_voltage">Molifying Volages</a></li> 
    </ol>
    <li> <a href="#complete">Training Complete Graph</a> 
    <ol>
        <li> <a href="#multi_function">Loss Curves Multi Function</a></li> 
    </ol>
    <li> <a href="#xor">XOR Task</a> 
    <li> <a href="#compute_mollified">Backpropogation on Molified Loss</a>
    <li> <a href="#mnist">Real Application: MNIST</a>
    <li> <a href="#signal">Signal Processing Approaches</a>
    <li> <a href="#refs">Useful References</a>
</ol>
<br>

<h2 id="intro">Introduction</h2>
<p>
My research is currently focused on training network of biological neurons (BNNs) on machine learning (ML) tasks. In <a href="./bnns_spiking_data.html">another post</a>, I dive into a specific task well suited for BNNs: training BNNs on spiking data. In that work, I encountered a number of issues. One issue was designing a clear loss reflecting the desired solution of the BNN. I found that the <a href="#loss_landscape_def">loss landscape*</a> exhibited discontinuities and very chaotic gradients at the micro scale but had structure at the macro scale. This blog post focuses on analyzing loss landscapes for some simple examples with the aim of building up some understanding and proposing solutions to facilitate training in such contexts.
</p>

<br>

<p style="font-size: 0.55cm;"><u id="loss_landscape_def">* The loss landscape</u> is a surface of losses on a subset of the data resulting from small perturbations in network parameters. For example, if the network, \(N_w\), just has one parameter, \(w\), then the loss landscape is the set \(\{\text{loss}(N_{w'}(X),Y^*) | w - \delta w \leq w' \leq w + \delta w\},\) where \(\delta w\) is specified and \(X, Y^*\) are the inputs and targets, respectively, for the subset of samples.
</p>


<h2 id="simple">Most Simple Case</h2>
<p>
The simplest case I considered here was simply turning a single <a href="https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model">Hodgkin-Huxley</a> (HH) neuron off, so that it does not spike anymore. Let \(\eta = (V, m, n, h)\) denote the neuron state which is composed of voltage and gating variables. If \(a(t)\) is the output of the neuron at time \(t\), then we can let the loss just be the average output over some time interval \(\tau\):
$$L(\eta) = \frac{1}{\tau} \int_0^\tau a(t) dt.$$
Ultimately, we aim to minimize \(L\), meaning that the neuron outputs as little as possible.
</p>

<h2 id="loss_simple">Loss Landscape</h2> 
<p>

First, let's generate the loss landscape by running a single neuron trial with a time interval \(\tau = 1\) second, and evaluating the final loss. Varying the single weight parameter \(w\) gives the following results, where blue denotes loss and orange denotes number of firings:
<img src="./simple_no_grad.png" style="max-width:80%; width:60%;" alt="Full Single Trial"/>
Note that around \(w = 1.8\) is where the neuron undergoes a bifurcation and the behavior changes, leading to no firing. This is due to the fact that a neuron can only fire so many times within a given interval. 
</p>
<p>
We can also look at a subset of the plot for more clear detail. Here is a zoom in on the range \(w \in [0.5, 0.75]\):
<img src="./simple_no_grad_0.5_0.75.png" style="max-width:80%; width:60%;" alt="Zoom Single Trial"/>
</p>
<br>
<p>
These plots demonstrate some key problems with the task and performing gradient descent. In particular, the zoom in shows that the loss changes chaotically on the micro scale and any approximate gradients would have to be almost infinite to approximate these changes. Furthermore, there are large discontinuous jumps in the loss when a the number of firings changes. Regarding the former problem, it might seem that we can increase the resolution very high so that the micro changes are more smooth. However, increasing the resolution cannot fix the latter problem because changes in firing still cause discontinuous jumps no matter what the resolution because this is an integer valued discontinuous function itself.
</p>
<p>
At this point, the goal may seem hopeless. How can we get gradients from something so chaotic? Let's understand what is causing the issues. Note that we are using a fixed timeframe \(\tau\) which may not be sufficiently long. Even worse, we are only using a single neuron to evaluate the loss curve. If we let \(N_{trials}\) denote the number of neurons we use to sample the loss, in the limit \(\tau, N_{trials} \rightarrow \infty\) the big varied jumps in the loss curve should go away. So, let's add noise to each neuron (with fixed initial conditions for now) and increase \(N_{trials}\). If we let \(N_{trials} = 10\), below is the resulting loss curve:
</p>
<img src="./simple_0_2_10_trials.png" style="max-width:80%; width:60%;" alt="Full Single Trial"/>
<p>
As can be seen, the discontinuities are much less extreme. Let's also zoom in as we did above:
</p>
<img src="./simple_0.5_0.75_10_trials.png" style="max-width:80%; width:60%;" alt="Full Single Trial"/>
<p>
Wow! A curve that previously looked like a discontinuous mess now appears to be almost completely straight! This makes sense because each of the 10 trial neurons fires at different times and so the effects of firings are much less abrupt for a single trial. We can still see some high frequency noise in the curve but otherwise it looks much better.
</p>

<h2 id="gradients_simple">Gradient Descent on This Landscape</h2>

<h3>Approximate Approach</h3>

<p>
The next step in analysis of this very simple case is actually trying out gradient descent as an optimization method. Firstly, let's try a finite difference approach based on the precomputed loss curve. Suppose we are given the loss curve as a set:
$$\{(w_i, L(w_i)) | w_{i+1} = w_i + \Delta w, i=1...n\}.$$
Instead of automatically differentiating to compute gradients of \(L\) with respect to \(w\), we can just use a centered difference approximation:
$$\frac{\partial L}{\partial w_i} \approx \frac{L(w_{i+1}) - L(w_{i-1})}{2 \Delta w}.$$
We can then use this approximate derivative to do a gradient descent step:
$$w \mapsto w - \eta \frac{\partial L}{\partial w}.$$
Finally, since we are operating on the set of points \(w_i\), we can round to the nearest gridpoint (which should not be a big change with a high resolution grid). 
</p>
<p>
Let's see what happens with varied initial values of \(w\) and learning rates \(\eta\). First, below is a plot with initial \(w = 1.5, \eta = 0.1\):
<img src="./approx_grad_descent_1.5_0.1.png" style="max-width:50%; width:50%;" alt="Grad Approx 1"/>
Note that the gradient descent gets "stuck" because it keeps jumping between two values. Here is the same case but with \(\eta = 0.2\):
<img src="./approx_grad_descent_1.5_0.2.png" style="max-width:50%; width:50%;" alt="Grad Approx 2"/>
In this case, one of the approximate gradients is simple 0 (i.e. the curve is flat at this point) so it gets stuck again. 
</p>
<p>
Things are not looking that good. The issue is that at the micro scale there is a lot of noise in the curve even though it posses a tangent that is quite clear without the noise. This suggests a possible fix: <i>let's try using multiple sample points to compute the approximate tangent</i>. To do so, we can select points locally to the left and right of the sample point then use a linear regression fit to approximate the tangent. Here is an example of such fitting at a point on the loss plot with 5 neighbors sampled to both the left and right:
<img src="./idea.png" style="max-width:50%; width:50%;" alt="Idea"/>
</p>
<p>
If we use an adaptive value of \(\eta\) that grows over learning (so that we don't get stuck as above) and the linear regression approach with 10 local neighbors, we get convergence:
<img src="./approx_grad_descent_1.5_0.05_True.png" style="max-width:50%; width:50%;" alt="Grad Approx Linear Regression"/>
</p>

<h3>Automatic Approach</h3>
<p>text here</p>

<h3 id="molification">Molification</h3>

<p>
One possible avenue for generating loss landscapes that are more easy to do gradient descent on is "molification." Molification is the application of a "molifier" to a badly behaving function (e.g. very jagged with discontinuities) through an operation such as convolution. A molifier is defined by the following criteria:
</p>

<br>
<center style="font-size: 0.6cm;">
$$\text{Definition (molifier): } \phi \text{ is a smooth function on } \mathbb{R}^n \text{ satisfying: }$$
1. <a href="https://en.wikipedia.org/wiki/Support_(mathematics)">Compact support</a>, which essentially just guarantees that is is only locally applied, 
$$2. \int_{\mathbb{R}^n} \phi(x) dx = 1, \text{ so it is like a distribution and it does not change scaling when we convolve it,}$$
$$3. \lim_{\epsilon \rightarrow 0} \epsilon^{-n} \phi(x/\epsilon) = \delta(x) \text{ (dirac delta function)}.$$
</center>
<br>

<p>
I intuitively think of the final criteria as requiring that the function "looks like" a discontinuous jump at the \(n\)th derivative scale. Here is an example of a function undergoing progressive molification from Wikipedia (which, incidentally, I think they used ray-tracing to produce):
</p>
<br>
<img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Heat_eqn.gif" alt="Molificiation" style="width:40%;"/>
<br>
<p>
The canonical molifier is the bump function \(\phi(x)\) on \(\mathbb{R}^n\) defined by 
$$ \phi(x) = \begin{cases} k_n e^{\frac{1}{|x|^2 - 1}} & |x| \lt 1 \\ 0 & |x| \geq 1 \end{cases},$$
where \(k_n\) is chosen to be one over the volume so that condition (2) above is satisfied:
</p>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Mollifier_Illustration.svg/1920px-Mollifier_Illustration.svg.png" alt="Bump 1d"/>
<p>
Note that is might be tempting to use a function like a Gaussian blob as a molifier, but such a function is not compactly supported even though is gets very small as we let \(x\) grow. 
</p>

<h3 id="molify_loss">Applying to BNN</h3>

<p> 
There are two conceivable ways of applying molification to our task. Firstly, we can simply apply molification to the final loss function. This has a more straight-forward effect on the loss but it has the disadvantage of requiring us to compute all losses in a local neighborhood to perform molification at one point. Secondly, we could apply molification over time to the voltage trace outputted from a particular neuron. The idea with this is to "smooth" out the spikes leading to better gradients and is very related to the <a href="https://arxiv.org/pdf/1901.09948.pdf">surrogate-gradients</a> approach. The main difference is that in these approaches they are working with discontinuous neurons (leaky-integrate-and-fire) and most of the papers I have seen using this approach make LIF neurons continuous by completely disregarding gradients when spikes occur. 
</p>

<h3>Approach 1: Molifying Loss</h3>

<p>
Here is a molifier using the scaled bump from above, where \(c\) is chosen to be some small value:
</p>

<img src="./molifier_voltage.png" alt="Voltage Molifier" style="width:40%;"/>

<p>If we molify the loss directly it removes the noise at the small scale and makes the loss curve much more tractable for gradient descent:</p>

<br>
<center>
<div style="max-width:50%; width:40%; display:inline-block; ">
<b style="font-size:0.7cm;">Molified Loss Curve</b>
<img src="losses_mollified.png" style="max-width:100%;" alt="Molified Loss Curve"/>
</div>
<div  style="max-width:50%; width:40%;  display:inline-block;">
<b style="font-size:0.7cm;">Zoom (Orange=Molified, Blue=Original)</b>
<img src="losses_mollified_zoom.png" style="max-width:100%;" alt="Zoom "/>
</div>
</center>
<br>

<p>If we apply gradient descent as above to this, we get convergence faster and much more clear gradients. This is seen below. Approximate gradients with a finite difference are used (no linear regression unlike above), and the learning rate is adaptively scaled up over epochs:

</p>
<br>
<center>
<div style="max-width:50%; width:40%; display:inline-block; ">
<b style="font-size:0.7cm;">Without Molification</b>
<img src="approx_grad_descent_1.5_0.05_False_False.png" style="max-width:100%;" alt="Without Molification"/>
</div>
<div  style="max-width:50%; width:40%;  display:inline-block;">
<b style="font-size:0.7cm;">With Molification</b>
<img src="approx_grad_descent_1.5_0.05_False_True.png" style="max-width:100%;" alt="With Molification"/>
</div>
</center>
<br>

<img src="./molifier_2d.png" style="max-width:50%; width:50%;" alt="Idea"/>

<h3 id="molify_voltage">Approach 2: Molifying Voltage</h3>

<p>Here is a raster plot and some example traces of 1000 neuron voltages with random noise and no transient and with the same input weight. 10,000 timesteps were used for simulation:</p>

<center>
<div style="max-width:50%; width:40%; display:inline-block; ">
<img src="./voltages_imshow.png" style="max-width:100%;"  alt="Voltages Raster"/>
</div>
<div  style="max-width:50%; width:40%;  display:inline-block;">
<img src="./voltages_example.png" style="max-width:100%;"  alt="Voltages Raster"/>
</div>
</center>
<br>

<p>
Let's try molifying the voltages. First, let \(t_0\) denote the time of the first spike occurence for the first voltage trace above. Let's zoom in around \(t_0\) for this voltage:
</p>

<img src="./voltage_zoom.png" alt="Voltage Zoom"/>

<p>
If we apply the same molifier as above with appropriate \(c\) (I chose it to be 10 timesteps), repeated application gives these results: 
</p>

<img src="./mollified_voltage.png" alt="Voltage Molified"/>

<p>
Note there is an issue at the borders because at these edges we are trying to do a convolution so we need to use "ghost points" that do not exist. This is not an issue in practice because we can just molify the full voltage trace over many timesteps and truncate the transient and final few timesteps. 
</p>

<h3 id="xor">XOR Task</h3>

<img src="./xor.png" alt="XOR Net" style="width:50%;"/>

<h3 id="compute_mollified">Backpropogation on Molified Loss</h3>

<p>
I was suggested to look at the "log-likelihood trick" outlined <a href="https://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">here</a> to compute the mollified loss gradient. I will work through the math here; it seems very promising. 
</p>
<br>
<hr>
<h4 style="margin-top: 0px;">Notation</h4>
<p>
Firstly, some notation. Let's denote the parameters of the network by \(\theta\) and the network as a function \(N_\theta(x)\), where \(x\) is the input sample or mini-batch of samples. Next, let's denote a distribution (e.g. Gaussian blob) at \(\theta\) in the parameter space by \(p_\theta(v)\). We draw parameters \(v \in \mathbb{R}^m\) for the network from this distribution, where \(m\) is the number of parameters (weights).
</p>
<br>
<hr>
<p>
Now, the smoothed loss can be expressed as an expected value over the distribution, or in simpler words, a weighted sum in some neighborhood. In particular, the smoothed loss looks like:
$$\mathcal{L}^{smooth}_\theta(x) = \int_{v \in \mathbb{R}^m} p_\theta(v) f(N_v(x)) dv.$$
</p>
<p>
Then, let's differentiate with respect to the parameters \(\theta\). Since we are thinking of parameters as lying in \(\mathbb{R}^m\), then the derivative we need for gradient descent is just a vector in \(\mathbb{R}^m\) and is the gradient: 
$$\nabla_\theta \text{ } \mathcal{L}^{smooth}_\theta(x) = \nabla_\theta \text{ } \int_{v \in \mathbb{R}^m} p_\theta(v) f(N_v(x)) dv.$$
Assuming Leibniz rule holds, this is 
$$= \int_{v \in \mathbb{R}^m} (\nabla_\theta \text{ } p_\theta(v)) f(N_v(x)) dv$$
It might seem like we need to gradient the second term also, but if you look at Leibniz rule this is not the case since \(f(N_v(x))\) depends on \(v\), not \(\theta\). Note the advantage of the above term: <i>we do not need to backpropogate on the network to compute it!!</i>. This means we can avoid any complexity involved with backpropogation on the network such as non-differentiability, etc! This seems like a massive advantage in my opinion.  
</p>
<p>
Finally, let's finish up by making the above integral more feasible to compute. In particular, let's multiply by one then use the log derivative identity: 
$$= \int_{v \in \mathbb{R}^m} \frac{p_\theta(v)}{p_\theta(v)} (\nabla_\theta \text{ } p_\theta(v)) f(N_v(x)) dv$$ 
$$= \int_{v \in \mathbb{R}^m} p_\theta(v) \cdot [\nabla_\theta \log(p_\theta(v))] f(N_v(x)) dv$$ 
$$= \mathbb{E}_{p_\theta(v)}[f(N_v(x)) \nabla_\theta \log(p_\theta(v))].$$
To compute this, we can use a Monte-Carlo estimate like in the blog I linked above: 
$$\approx \frac{1}{S} \sum_{s=0}^S f(N_{v^{(s)}}(x)) \nabla_\theta \log(p_\theta(v^{(s)})),$$
where we draw the random parameters \(v^{(s)}\) from this Gaussian blob (this can be done practically by just adding a noise term to weights in each layer of the network!).
</p>

<h4>Quick Note: Computing Log of Distribution is Usually Nicer</h4>

<p> 
This is not particularly important in the case of BNNs, because computing the BNN is much harder than computing a single distribution, but it should be noted that one reason why the log trick is used is because the log derivative term is usually much simpler than the original distribution computation. This is very clear in the example of a 1 dimensional normal distribution given by:
$$p_\theta(v) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} (\frac{v - \theta}{\sigma})^2}.$$
When we take a log of this, the exponential cancel out:
$$\log p_\theta(v) = \log(-\sigma \sqrt{2 \pi}) -\frac{1}{2} (\frac{v - \theta}{\sigma})^2.$$
Finally, when we take the derivative with respect to \(\theta\), we get the really simple term:
$$\frac{\partial}{\partial \theta} \log p_\theta(v) = \frac{v - \theta}{\sigma^2}.$$
So, this elegant term is just a linear function measuring the signed distance from the mean \(\theta\) divided by the variance.

<h3>Related References for Monte-Carlo</h3>

<p> 
The next step with the Monte-Carlo approach or integration approach is to compare them and see which works better. It appears that the integration approach is more accurate for smaller number of samples in 1D. For example, the plot below uses the same number of samples (100) for the integration approach and Monte-Carlo log approach to compute the smoothed gradients at each points from -10 to 10 of some toy example loss. As can be seen, the fact that the loss grows quadratically on the borders makes the Monte-Carlo also blow up in these regions but does not affect the integral approach.

<b style="font-size: 2cm;">NOTE: It seems like what we are computing is a value function</b>
</p>

<br>
<hr style="margin-bottom:0px;">
<img src="./smoothed_log_trick.png" style="max-width:100%; width:100%; margin-top:0px;" alt="Computing Gradients"/>

<br>

<img src="./log_trick_distributions.png" style="max-width:100%; width:100%;" alt="Distributions"/>

<br>

<img src="./log_trick_convergence.png" style="max-width:100%; width:80%;" alt="Error"/>

<br>
<hr>


<p>
I did find that if we increase the number of samples \(S\) for the log trick sufficiently high (e.g. 1,000 samples) it does converge to the right thing. The above might suggest that integration methods are better in this case. However, the integration approach does not generalize well when we use a real network parameter set that could be millions of dimensions. On the other hand, it appears from my preliminary search of what is available around that Monte-Carlo avoids the "curse of dimensionality" meaning hopefully that we can use it in many dimensions. We will still have to figure out a way of using less samples, but if the number of samples does not depend on the dimension this is certainly very promising. I also found that there are methods called <i>sparse grid methods</i> that do integration in a way that avoids dimensionality issues. Here are some random sources from the internet, mostly just so I don't lose them:
</p>
<br>
<ol>
    <li><a href="http://www.cs.toronto.edu/~tang/papers/sfnn.pdf">Stochastic Neural Networks</a></li>
    <li><a href="https://math.stackexchange.com/questions/88262/why-monte-carlo-integration-is-not-affected-by-curse-of-dimensionality">Why Monte Carlo not affected by curse of dimensionality Stack Overflow post</a></li>
    <li><a href="https://math.stackexchange.com/questions/49655/pde-feynman-kac-vs-finite-difference-methods/49835#49835">A similar Stack Overflow post with sources</a></li>
    <li><a href="https://en.wikipedia.org/wiki/Sparse_grid">Sparse Grid Wikipedia</a></li>
</ol>

<h4>Todo Investigation Topics for This Approach</h4>

<p>
<ol>
	<li>Monte-Carlo sampling and error bounds</li>
	<li>Importance sampling</li>
	<li>Sparse grid</li>
    <li>Reinforce algorithm and policy gradients</li>
    <li>Q-learning</li>
    <li>Applying to: BNN, LIF network, binary network, stochastic network</li>
</ol>
</p>

<h3 id="complete">Training Complete Graph</h3>

<p>
Above we looked at the simple task of turning off a single neuron. We can extend this to many neurons in a number of different architectural ways. For example, we can create a feedforward network, a chain of \(n\) neurons, \(n\) independent neurons with input, or, in perhaps the most complex case, a complete graph on \(n\) neurons. Let's consider the latter case in this section. Here is what I mean by a complete graph (on 5 nodes in this case): 
</p>

<img src="./dot_1.png" alt="Net 1"/>

<p>
Note that we include connections from a neuron to itself. We can connect up multiple complete graphs and add input output neurons to make things more complex, but for now, let's just consider an isolated complete graph with no input output. As above, let's consider the task of turning of the neurons, so we can let the loss be the mean output of all the neurons averaged over the timeframe. 
</p>

<p>
One advantage I will mention related to the complete graph case is that the loss landscape is quite easy to visualize. In particular, if we have a lot of neurons and randomized connections from some distribution, because of the symmetry inherent in the complete graph, <i>the loss landscape is essentially one dimensional</i> with noisiness in each individual weight. 
</p>

<h3 id="multi_function">Loss Curves Multi Function</h3> 

<p>
Let's analyze the loss in this more complex case. Even in the single neuron "complete graph" the loss is much more complicated. This case different from the case above because instead of the neuron getting a uniform input, it gets input from itself in a feedback loop:
</p>
<br>

<img src="./dot_2.png" alt="Single Complete Graph" style="width:10%;"/>

<p>
Here is the loss curve:
</p>

<img src="./loss_full_complete_5TRIAL.png" alt="Single Complete Graph Loss" style="width:50%;"/>

<p> 
As we can see, it appears extremely noisy. It turns out that the noise is due to multiple different "bands" of loss curves, which is clear if we zoom in and sample a 1000 points:
</p>

<img src="./loss_snapshot_complete_std_1.0.png" alt="Single Complete Graph Loss" style="width:50%;"/>

<p>
In this case, the loss really takes the form of a <a href="https://en.wikipedia.org/wiki/Multivalued_function">"multi function"</a> and the notion of a derivative is essentially useless in its most direct form because the loss may jump between one of many different curves discontinuously! We can fit a linear regression, like above, to get a better estimate of a gradient (like the orange line shown), but this also doesn't work particularly well to capture the curve.
</p>

<h3 id="mnist">Real Application: MNIST</h3>

<h3>Convergence of Linear Regression Method</h3>

<p>
As a first pass I'll use the linear regression method (which is easier to implement because there are multiple alternate ways to implement the Gaussian smoothing approach). On my first pass directly comparing this method to autodiff in the case of a smooth loss, autodiff worked perfectly and the regression method did not work at all. Thus, it is necessary to look at why the latter fails. 
</p>

<h4>Linear Classifier</h4>

<p>
Let's first remove the dependence on a BNN and just use a linear classifier (i.e. a neural network with no activation functions). The architecture I used is as follows:
</p>
<br>
<img src="./mnist_arch.png" style="max-width:80%; width:40%;" alt="MNIST Arch"/>
<p>
Note that we cannot easily visualize the full loss landscape in this setting. However, we can predict exact gradients using automatic differentiation and compare these to those computed using linear regression. This should allow us to get an idea of how the linear regression method can converge for different sample sizes and standard deviations. 
</p>
<p>
Let's first work on an extremely simple case: let the input to the network be uniformly 1 and the loss function just be the mean output, so we want an output that is as negative as possible. In this case, the loss can be computed directly:
$$z_1 = W_1 \cdot \mathbb{1},$$
$$z_2 = W_2 \cdot z_1,$$
$$L(z_2) = \text{avg}(z_2).$$
Note then that the Jacobian of the loss with respect to \(W_2\) is just a matrix where each column is equal to the average of \(z_2\) (this can be derived by hand by looking at the derivative with respect to an individual weight in \(W_2\)). Here's some empirical proof:
</p>

<br>
<center>
<div style="max-width:50%; width:49%; display:inline-block; ">
<b style="font-size:0.7cm;">Start of Autodiff Learning</b>
<img src="grad_True_0.png" style="max-width:100%;" alt="Start"/>
</div>
<div  style="max-width:50%; width:49%;  display:inline-block;">
<b style="font-size:0.7cm;">End of Autodiff Learning</b>
<img src="grad_True_40.png" style="max-width:100%;" alt="End"/>
</div>
</center>
<br>

<p>
When we apply the regression method, the gradients have no such structure: they appear to be essentially drawn from a random distribution (this figure uses S = 2000, and a very small standard deviation of 0.00001):
</p>

<img src="./grad_naive_regression.png" style="max-width:80%; width:60%;" alt="Naive Regression"/>

<p>
I also noted that there seems to be an issue with scaling: as we increase \(S\), the standard deviation of the gradient seems to increase as well, which should not be the case when we use sufficiently large \(S\). Let's first focus on the more glaring issue of the lack of structure in the gradient. We should see that along each row the gradient is essentially constant. One hypothesis might be that we are using too small of an \(S\) parameter and thus the structure is not captured. One way to test these ideas is to isolate paricualr parameters and only look in these paramter directions. For example, we can look at the 2d curve formed by the loss given by varying two parameters (e.g on the same row versus on seperate rows). We could also try "coordinate descent" which is a variant of gradient descent that isolates different coordinate subsets each learning step and only works on these. Using a sufficiently small number of coordinates would allow us to have a more well-conditioned regression. 
</p>

<p>
Let's first look at how the loss varies along parciular parameter directions. Let's look along the first two entires in the first row of \(W_2\) and along the first two entires of the first column. In the former case, we should see that both parameters behave essentially the same, whereas in the the second case they should be independent. Let's look at the sampled loss for where we isolate one of the three parameters as the x-axis:
</p>


<img src="./losses_00_01_10.png" style="max-width:80%; width:60%;" alt="Naive Regression"/>

<p>
Wow, these results are terrible! Note that there is very little structure between the loss and the parameter suggesting that the choice of parameter has essentially zero effect on the loss! I hypothesize that this is likely due to the fact that we are adding noise to all the network parameters for each sample, so this noise might be making the sample loss very noisy and hides the actual structure. Let's try just vary the three parameters above in our regression:
</p>



<h3 id="signal">Signal Processing Approach</h3>

<p>
This research is very open and I am considering a number of approaches. One that seems like a promising approach is using signal processing methods for training BNNs. This section will explore this.
</p>

<h4>Cross-Correlation</h4>

<p>
For real or complex functions \(f, g\) on some domain \(D\), their cross-correlation is defined as:
$$(f \star g)(\tau) := \int_D \overline{f(t)} g(t + \tau) dt = \int_D \overline{f(t - \tau)} g(t) dt,$$
where \(\overline{f}\) is the complex conjugate and \(\tau\) is called the "lag". To understand this definition, if \(g(t)\) is a discrete signal and is just \(f(t)\) but shifted to the right by \(\tau\), then \(f \star g\) will be zero everywhere except at \(\tau = 1\), where it will be the integral of \(|f(t)|^2\), which is the squared <a href="https://mathworld.wolfram.com/L2-Norm.html">L^2 norm</a> of \(f(t)\). Note that if \(g, f\) are not signals and are continuously varying, then the cross-correlation will not be zero outside but will decay outside 1 and have a max at 1.
</p>

<p>
Cross-correlation loss:
$$\text{Loss} = \int_0^T y(t) y^*(t + \tau) dt.$$
</p>

<h3 id="refs">Useful References</h3>
<ol>
    <li><a href="https://arxiv.org/pdf/1706.04698.pdf">Gradient Descent for Spiking Neural Networks</a></li>
    <li><a href="https://arxiv.org/pdf/1901.09948.pdf">Surrogate Gradient Learning in Spiking Neural Networks</a></li>
    <li><a href="https://nowak.ece.wisc.edu/ece830/ece830_spring13_adaptive_filtering.pdf">Adaptive Filtering</a></li>
    <li><a href="https://arxiv.org/pdf/1712.09913.pdf">Visualizing the Loss Landscape of Neural Networks</a></li>
    <li><a href="https://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">Machine Learning Trick of the Day (5): Log Derivative Trick</a></li>
</ol>






<!--<img src="./voltage.png" style="max-width:100%; width:100%;" alt="Voltage"/>-->


<div class="top_bar" style="left: 30px;">
<a href="index.html" style="text-decoration: none;">goto: main</a>
</div>

<div class="top_bar" style="right: 30px;">
<a href="" style="text-decoration: none;">goto: top</a>
</div>

</body>
</html>
