<html>
<head>
<meta name="description" content="James Hazelden">
<link rel=StyleSheet href="nut_styles.css" type="text/css" media=all>
<title>James Hazelden</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<link rel="icon" href="./koch-snowflake.png" type="image/x-icon">
<script language="JavaScript">
<!--
   if (self.location.href != top.location.href) {
      top.location.href = self.location.href;
   }
-->
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
</head>
<body>

<div class="main_text">
<h1>Loss Landscapes Emerging from BNNs</h1>
<hr>

<ol style="max-width:45%;">
    <li> <a href="#intro">Introduction</a> 
    <li> <a href="#simple">Most Simple Case</a> 
    <ol>
        <li> <a href="#loss_simple">Loss Landscape</a></li> 
        <li> <a href="#gradients_simple">Gradient Descent on This Landscape</a></li> 
    </ol>
    <li> <a href="#molification">Mollification</a> 
    <ol>
        <li> <a href="#molify_loss">Mollifying Loss Curve</a></li> 
        <li> <a href="#molify_voltage">Mollifying Volages</a></li> 
    </ol>
    <li> <a href="#complete">Training Complete Graph</a> 
    <ol>
        <li> <a href="#multi_function">Loss Curves Multi Function</a></li> 
    </ol>
    <!--    <li> <a href="#xor">XOR Task</a> -->
    <li> <a href="#compute_mollified">Computing Gradient of Molified Loss</a>
    <li> <a href="#mnist">Real Application: MNIST</a>
    <ol>
        <li> <a href="#linear_classifier">Linear Classifier</a></li> 
        <li> <a href="#ann">ANN</a></li> 
        <li> <a href="#bnn">BNN</a></li> 
        <ol>
            <li> <a href="#case_1_bnn">Case 1: Short Timeframe</a></li> 
            <li> <a href="#case_2_bnn">Case 2: Long Timeframe</a></li> 
        </ol>
    </ol>
    <li> <a href="#signal">Signal Processing Approaches</a>
    <li> <a href="#refs">Useful References</a>
</ol>
<br>

<h2 id="intro">Introduction</h2>
<p>
My research is currently focused on training network of biological neurons (BNNs) on machine learning (ML) tasks. In <a href="./bnns_spiking_data.html">another post</a>, I dive into a specific task well suited for BNNs: training BNNs on spiking data. In that work, I encountered a number of issues. One issue was designing a clear loss reflecting the desired solution of the BNN. I found that the <a href="#loss_landscape_def">loss landscape*</a> exhibited discontinuities and very chaotic gradients at the micro scale but had structure at the macro scale. This blog post focuses on analyzing loss landscapes for some simple examples with the aim of building up some understanding and proposing solutions to facilitate training in such contexts.
</p>

<br>

<p style="font-size: 0.55cm;"><u id="loss_landscape_def">* The loss landscape</u> is a surface of losses on a subset of the data resulting from small perturbations in network parameters. For example, if the network, \(N_w\), just has one parameter, \(w\), then the loss landscape is the set \(\{\text{loss}(N_{w'}(X),Y^*) | w - \delta w \leq w' \leq w + \delta w\},\) where \(\delta w\) is specified and \(X, Y^*\) are the inputs and targets, respectively, for the subset of samples.
</p>


<h2 id="simple">Most Simple Case</h2>
<p>
The simplest case I considered here was simply turning a single <a href="https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model">Hodgkin-Huxley</a> (HH) neuron off, so that it does not spike anymore. Let \(\eta = (V, m, n, h)\) denote the neuron state which is composed of voltage and gating variables. If \(a(t)\) is the output of the neuron at time \(t\), then we can let the loss just be the average output over some time interval \(\tau\):
$$L(\eta) = \frac{1}{\tau} \int_0^\tau a(t) dt.$$
Ultimately, we aim to minimize \(L\), meaning that the neuron outputs as little as possible.
</p>

<h2 id="loss_simple">Loss Landscape</h2> 
<p>

First, let's generate the loss landscape by running a single neuron trial with a time interval \(\tau = 1\) second, and evaluating the final loss. Varying the single weight parameter \(w\) gives the following results, where blue denotes loss and orange denotes number of firings:
<img src="./simple_no_grad.png" style="max-width:80%; width:60%;" alt="Full Single Trial"/>
Note that around \(w = 1.8\) is where the neuron undergoes a bifurcation and the behavior changes, leading to no firing. This is due to the fact that a neuron can only fire so many times within a given interval. 
</p>
<p>
We can also look at a subset of the plot for more clear detail. Here is a zoom in on the range \(w \in [0.5, 0.75]\):
<img src="./simple_no_grad_0.5_0.75.png" style="max-width:80%; width:60%;" alt="Zoom Single Trial"/>
</p>
<br>
<p>
These plots demonstrate some key problems with the task and performing gradient descent. In particular, the zoom in shows that the loss changes chaotically on the micro scale and any approximate gradients would have to be almost infinite to approximate these changes. Furthermore, there are large discontinuous jumps in the loss when a the number of firings changes. Regarding the former problem, it might seem that we can increase the resolution very high so that the micro changes are more smooth. However, increasing the resolution cannot fix the latter problem because changes in firing still cause discontinuous jumps no matter what the resolution because this is an integer valued discontinuous function itself.
</p>
<p>
At this point, the goal may seem hopeless. How can we get gradients from something so chaotic? Let's understand what is causing the issues. Note that we are using a fixed timeframe \(\tau\) which may not be sufficiently long. Even worse, we are only using a single neuron to evaluate the loss curve. If we let \(N_{trials}\) denote the number of neurons we use to sample the loss, in the limit \(\tau, N_{trials} \rightarrow \infty\) the big varied jumps in the loss curve should go away. So, let's add noise to each neuron (with fixed initial conditions for now) and increase \(N_{trials}\). If we let \(N_{trials} = 10\), below is the resulting loss curve:
</p>
<img src="./simple_0_2_10_trials.png" style="max-width:80%; width:60%;" alt="Full Single Trial"/>
<p>
As can be seen, the discontinuities are much less extreme. Let's also zoom in as we did above:
</p>
<img src="./simple_0.5_0.75_10_trials.png" style="max-width:80%; width:60%;" alt="Full Single Trial"/>
<p>
Wow! A curve that previously looked like a discontinuous mess now appears to be almost completely straight! This makes sense because each of the 10 trial neurons fires at different times and so the effects of firings are much less abrupt for a single trial. We can still see some high frequency noise in the curve but otherwise it looks much better.
</p>

<h2 id="gradients_simple">Gradient Descent on This Landscape</h2>

<h3>Approximate Approach</h3>

<p>
The next step in analysis of this very simple case is actually trying out gradient descent as an optimization method. Firstly, let's try a finite difference approach based on the precomputed loss curve. Suppose we are given the loss curve as a set:
$$\{(w_i, L(w_i)) | w_{i+1} = w_i + \Delta w, i=1...n\}.$$
Instead of automatically differentiating to compute gradients of \(L\) with respect to \(w\), we can just use a centered difference approximation:
$$\frac{\partial L}{\partial w_i} \approx \frac{L(w_{i+1}) - L(w_{i-1})}{2 \Delta w}.$$
We can then use this approximate derivative to do a gradient descent step:
$$w \mapsto w - \eta \frac{\partial L}{\partial w}.$$
Finally, since we are operating on the set of points \(w_i\), we can round to the nearest gridpoint (which should not be a big change with a high resolution grid). 
</p>
<p>
Let's see what happens with varied initial values of \(w\) and learning rates \(\eta\). First, below is a plot with initial \(w = 1.5, \eta = 0.1\):
<img src="./approx_grad_descent_1.5_0.1.png" style="max-width:50%; width:50%;" alt="Grad Approx 1"/>
Note that the gradient descent gets "stuck" because it keeps jumping between two values. Here is the same case but with \(\eta = 0.2\):
<img src="./approx_grad_descent_1.5_0.2.png" style="max-width:50%; width:50%;" alt="Grad Approx 2"/>
In this case, one of the approximate gradients is simple 0 (i.e. the curve is flat at this point) so it gets stuck again. 
</p>
<p>
Things are not looking that good. The issue is that at the micro scale there is a lot of noise in the curve even though it posses a tangent that is quite clear without the noise. This suggests a possible fix: <i>let's try using multiple sample points to compute the approximate tangent</i>. To do so, we can select points locally to the left and right of the sample point then use a linear regression fit to approximate the tangent. Here is an example of such fitting at a point on the loss plot with 5 neighbors sampled to both the left and right:
<img src="./idea.png" style="max-width:50%; width:50%;" alt="Idea"/>
</p>
<p>
If we use an adaptive value of \(\eta\) that grows over learning (so that we don't get stuck as above) and the linear regression approach with 10 local neighbors, we get convergence:
<img src="./approx_grad_descent_1.5_0.05_True.png" style="max-width:50%; width:50%;" alt="Grad Approx Linear Regression"/>
</p>

<h3 id="molification">Mollification</h3>

<p>
One possible avenue for generating loss landscapes that are more easy to do gradient descent on is "molification." Mollification is the application of a "molifier" to a badly behaving function (e.g. very jagged with discontinuities) through an operation such as convolution. A molifier is defined by the following criteria:
</p>

<br>
<center style="font-size: 0.6cm;">
$$\text{Definition (molifier): } \phi \text{ is a smooth function on } \mathbb{R}^n \text{ satisfying: }$$
1. <a href="https://en.wikipedia.org/wiki/Support_(mathematics)">Compact support</a>, which essentially just guarantees that is is only locally applied, 
$$2. \int_{\mathbb{R}^n} \phi(x) dx = 1, \text{ so it is like a distribution and it does not change scaling when we convolve it,}$$
$$3. \lim_{\epsilon \rightarrow 0} \epsilon^{-n} \phi(x/\epsilon) = \delta(x) \text{ (dirac delta function)}.$$
</center>
<br>

<p>
I intuitively think of the final criteria as requiring that the function "looks like" a discontinuous jump at the \(n\)th derivative scale. Here is an example of a function undergoing progressive molification from Wikipedia (which, incidentally, I think they used ray-tracing to produce):
</p>
<br>
<img src="https://upload.wikimedia.org/wikipedia/commons/a/a9/Heat_eqn.gif" alt="Mollificiation" style="width:40%;"/>
<br>
<p>
The canonical molifier is the bump function \(\phi(x)\) on \(\mathbb{R}^n\) defined by 
$$ \phi(x) = \begin{cases} k_n e^{\frac{1}{|x|^2 - 1}} & |x| \lt 1 \\ 0 & |x| \geq 1 \end{cases},$$
where \(k_n\) is chosen to be one over the volume so that condition (2) above is satisfied:
</p>
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Mollifier_Illustration.svg/1920px-Mollifier_Illustration.svg.png" alt="Bump 1d"/>
<p>
Note that is might be tempting to use a function like a Gaussian blob as a molifier, but such a function is not compactly supported even though is gets very small as we let \(x\) grow. 
</p>

<h3 id="molify_loss">Applying to BNN</h3>

<p> 
There are two conceivable ways of applying molification to our task. Firstly, we can simply apply molification to the final loss function. This has a more straight-forward effect on the loss but it has the disadvantage of requiring us to compute all losses in a local neighborhood to perform molification at one point. Secondly, we could apply molification over time to the voltage trace outputted from a particular neuron. The idea with this is to "smooth" out the spikes leading to better gradients and is very related to the <a href="https://arxiv.org/pdf/1901.09948.pdf">surrogate-gradients</a> approach. The main difference is that in these approaches they are working with discontinuous neurons (leaky-integrate-and-fire) and most of the papers I have seen using this approach make LIF neurons continuous by completely disregarding gradients when spikes occur. 
</p>

<h3>Approach 1: Mollifying Loss</h3>

<p>
Here is a molifier using the scaled bump from above, where \(c\) is chosen to be some small value:
</p>

<img src="./molifier_voltage.png" alt="Voltage Mollifier" style="width:40%;"/>

<p>If we molify the loss directly it removes the noise at the small scale and makes the loss curve much more tractable for gradient descent:</p>

<br>
<center>
<div style="max-width:50%; width:40%; display:inline-block; ">
<b style="font-size:0.7cm;">Mollified Loss Curve</b>
<img src="losses_molified.png" style="max-width:100%;" alt="Mollified Loss Curve"/>
</div>
<div  style="max-width:50%; width:40%;  display:inline-block;">
<b style="font-size:0.7cm;">Zoom (Orange=Mollified, Blue=Original)</b>
<img src="losses_molified_zoom.png" style="max-width:100%;" alt="Zoom "/>
</div>
</center>
<br>

<p>If we apply gradient descent as above to this, we get convergence faster and much more clear gradients. This is seen below. Approximate gradients with a finite difference are used (no linear regression unlike above), and the learning rate is adaptively scaled up over epochs:

</p>
<br>
<center>
<div style="max-width:50%; width:40%; display:inline-block; ">
<b style="font-size:0.7cm;">Without Mollification</b>
<img src="approx_grad_descent_1.5_0.05_False_False.png" style="max-width:100%;" alt="Without Mollification"/>
</div>
<div  style="max-width:50%; width:40%;  display:inline-block;">
<b style="font-size:0.7cm;">With Mollification</b>
<img src="approx_grad_descent_1.5_0.05_False_True.png" style="max-width:100%;" alt="With Mollification"/>
</div>
</center>
<br>

<p>
Another approximate notion of mollification is bluring is a distribution. This is not real mollification because some distributions (e.g. Gaussian) do not have compact support. However, outside of a sufficiently large sphere, they are almost exactly zero and so we can think of them as mollifying functions. Below is an example of a 2D Gaussian blob. As we will see below, using a distribution for mollification allows us to neatly mathematically work with the loss using expected values.
</p>

<img src="./molifier_2d.png" style="max-width:50%; width:50%;" alt="Idea"/>

<h3 id="molify_voltage">Approach 2: Mollifying Voltage</h3>

<p>Here is a raster plot and some example traces of 1000 neuron voltages with random noise and no transient and with the same input weight. 10,000 timesteps were used for simulation:</p>

<center>
<div style="max-width:50%; width:40%; display:inline-block; ">
<img src="./voltages_imshow.png" style="max-width:100%;"  alt="Voltages Raster"/>
</div>
<div  style="max-width:50%; width:40%;  display:inline-block;">
<img src="./voltages_example.png" style="max-width:100%;"  alt="Voltages Raster"/>
</div>
</center>
<br>

<p>
Let's try molifying the voltages. First, let \(t_0\) denote the time of the first spike occurence for the first voltage trace above. Let's zoom in around \(t_0\) for this voltage:
</p>

<img src="./voltage_zoom.png" alt="Voltage Zoom"/>

<p>
If we apply the same molifier as above with appropriate \(c\) (I chose it to be 10 timesteps), repeated application gives these results: 
</p>

<img src="./molified_voltage.png" alt="Voltage Mollified"/>

<p>
Note there is an issue at the borders because at these edges we are trying to do a convolution so we need to use "ghost points" that do not exist. This is not an issue in practice because we can just molify the full voltage trace over many timesteps and truncate the transient and final few timesteps. 
</p>

<!--<h3 id="xor">XOR Task</h3>

<img src="./xor.png" alt="XOR Net" style="width:50%;"/>-->

<h3 id="compute_mollified">Computing Gradient of Molified Loss</h3>

<p>
I was suggested to look at the "log-likelihood trick" outlined <a href="https://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">here</a> to compute the mollified loss gradient. I will work through the math here; it seems very promising. 
</p>
<br>
<hr>
<h4 style="margin-top: 0px;">Notation</h4>
<p>
Firstly, some notation. Let's denote the parameters of the network by \(\theta\) and the network as a function \(N_\theta(x)\), where \(x\) is the input sample or mini-batch of samples. Next, let's denote a distribution (e.g. Gaussian blob) at \(\theta\) in the parameter space by \(p_\theta(v)\). We draw parameters \(v \in \mathbb{R}^m\) for the network from this distribution, where \(m\) is the number of parameters (weights).
</p>
<br>
<hr>
<p>
Now, the smoothed loss can be expressed as an expected value over the distribution, or in simpler words, a weighted sum in some neighborhood. In particular, the smoothed loss looks like:
$$\mathcal{L}^{smooth}_\theta(x) = \int_{v \in \mathbb{R}^m} p_\theta(v) f(N_v(x)) dv.$$
</p>
<p>
Then, let's differentiate with respect to the parameters \(\theta\). Since we are thinking of parameters as lying in \(\mathbb{R}^m\), then the derivative we need for gradient descent is just a vector in \(\mathbb{R}^m\) and is the gradient: 
$$\nabla_\theta \text{ } \mathcal{L}^{smooth}_\theta(x) = \nabla_\theta \text{ } \int_{v \in \mathbb{R}^m} p_\theta(v) f(N_v(x)) dv.$$
Assuming Leibniz rule holds, this is 
$$= \int_{v \in \mathbb{R}^m} (\nabla_\theta \text{ } p_\theta(v)) f(N_v(x)) dv$$
It might seem like we need to gradient the second term also, but if you look at Leibniz rule this is not the case since \(f(N_v(x))\) depends on \(v\), not \(\theta\). Note the advantage of the above term: <i>we do not need to backpropogate on the network to compute it!!</i>. This means we can avoid any complexity involved with backpropogation on the network such as non-differentiability, etc! This seems like a massive advantage in my opinion.  
</p>
<p>
Finally, let's finish up by making the above integral more feasible to compute. In particular, let's multiply by one then use the log derivative identity: 
$$= \int_{v \in \mathbb{R}^m} \frac{p_\theta(v)}{p_\theta(v)} (\nabla_\theta \text{ } p_\theta(v)) f(N_v(x)) dv$$ 
$$= \int_{v \in \mathbb{R}^m} p_\theta(v) \cdot [\nabla_\theta \log(p_\theta(v))] f(N_v(x)) dv$$ 
$$= \mathbb{E}_{p_\theta(v)}[f(N_v(x)) \nabla_\theta \log(p_\theta(v))].$$
To compute this, we can use a Monte-Carlo estimate like in the blog I linked above: 
$$\approx \frac{1}{S} \sum_{s=0}^S f(N_{v^{(s)}}(x)) \nabla_\theta \log(p_\theta(v^{(s)})),$$
where we draw the random parameters \(v^{(s)}\) from this Gaussian blob (this can be done practically by just adding a noise term to weights in each layer of the network!).
</p>

<h4>Quick Note: Computing Log of Distribution is Usually Nicer</h4>

<p> 
This is not particularly important in the case of BNNs, because computing the BNN is much harder than computing a single distribution, but it should be noted that one reason why the log trick is used is because the log derivative term is usually much simpler than the original distribution computation. This is very clear in the example of a 1 dimensional normal distribution given by:
$$p_\theta(v) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} (\frac{v - \theta}{\sigma})^2}.$$
When we take a log of this, the exponential cancel out:
$$\log p_\theta(v) = \log(-\sigma \sqrt{2 \pi}) -\frac{1}{2} (\frac{v - \theta}{\sigma})^2.$$
Finally, when we take the derivative with respect to \(\theta\), we get the really simple term:
$$\frac{\partial}{\partial \theta} \log p_\theta(v) = \frac{v - \theta}{\sigma^2}.$$
So, this elegant term is just a linear function measuring the signed distance from the mean \(\theta\) divided by the variance.

<h3>Related References for Monte-Carlo</h3>

<p> 
The next step with the Monte-Carlo approach or integration approach is to compare them and see which works better. It appears that the integration approach is more accurate for smaller number of samples in 1D. For example, the plot below uses the same number of samples (100) for the integration approach and Monte-Carlo log approach to compute the smoothed gradients at each points from -10 to 10 of some toy example loss. As can be seen, the fact that the loss grows quadratically on the borders makes the Monte-Carlo also blow up in these regions but does not affect the integral approach.

<b style="font-size: 2cm;">NOTE: It seems like what we are computing is a value function</b>
</p>

<br>
<hr style="margin-bottom:0px;">
<img src="./smoothed_log_trick.png" style="max-width:100%; width:100%; margin-top:0px;" alt="Computing Gradients"/>

<br>

<img src="./log_trick_distributions.png" style="max-width:100%; width:100%;" alt="Distributions"/>

<br>

<img src="./log_trick_convergence.png" style="max-width:100%; width:80%;" alt="Error"/>

<br>
<hr>


<p>
I did find that if we increase the number of samples \(S\) for the log trick sufficiently high (e.g. 1,000 samples) it does converge to the right thing. The above might suggest that integration methods are better in this case. However, the integration approach does not generalize well when we use a real network parameter set that could be millions of dimensions. On the other hand, it appears from my preliminary search of what is available around that Monte-Carlo avoids the "curse of dimensionality" meaning hopefully that we can use it in many dimensions. We will still have to figure out a way of using less samples, but if the number of samples does not depend on the dimension this is certainly very promising. I also found that there are methods called <i>sparse grid methods</i> that do integration in a way that avoids dimensionality issues. Here are some random sources from the internet, mostly just so I don't lose them:
</p>
<br>
<ol>
    <li><a href="http://www.cs.toronto.edu/~tang/papers/sfnn.pdf">Stochastic Neural Networks</a></li>
    <li><a href="https://math.stackexchange.com/questions/88262/why-monte-carlo-integration-is-not-affected-by-curse-of-dimensionality">Why Monte Carlo not affected by curse of dimensionality Stack Overflow post</a></li>
    <li><a href="https://math.stackexchange.com/questions/49655/pde-feynman-kac-vs-finite-difference-methods/49835#49835">A similar Stack Overflow post with sources</a></li>
    <li><a href="https://en.wikipedia.org/wiki/Sparse_grid">Sparse Grid Wikipedia</a></li>
</ol>

<h4>Todo Investigation Topics for This Approach</h4>

<p>
<ol>
	<li>Monte-Carlo sampling and error bounds</li>
	<li>Importance sampling</li>
	<li>Sparse grid</li>
    <li>Reinforce algorithm and policy gradients</li>
    <li>Q-learning</li>
    <li>Applying to: BNN, LIF network, binary network, stochastic network</li>
</ol>
</p>

<h3 id="complete">Training Complete Graph</h3>

<p>
Above we looked at the simple task of turning off a single neuron. We can extend this to many neurons in a number of different architectural ways. For example, we can create a feedforward network, a chain of \(n\) neurons, \(n\) independent neurons with input, or, in perhaps the most complex case, a complete graph on \(n\) neurons. Let's consider the latter case in this section. Here is what I mean by a complete graph (on 5 nodes in this case): 
</p>

<img src="./dot_1.png" alt="Net 1"/>

<p>
Note that we include connections from a neuron to itself. We can connect up multiple complete graphs and add input output neurons to make things more complex, but for now, let's just consider an isolated complete graph with no input output. As above, let's consider the task of turning of the neurons, so we can let the loss be the mean output of all the neurons averaged over the timeframe. 
</p>

<p>
One advantage I will mention related to the complete graph case is that the loss landscape is quite easy to visualize. In particular, if we have a lot of neurons and randomized connections from some distribution, because of the symmetry inherent in the complete graph, <i>the loss landscape is essentially one dimensional</i> with noisiness in each individual weight. 
</p>

<h3 id="multi_function">Loss Curves Multi Function</h3> 

<p>
Let's analyze the loss in this more complex case. Even in the single neuron "complete graph" the loss is much more complicated. This case different from the case above because instead of the neuron getting a uniform input, it gets input from itself in a feedback loop:
</p>
<br>

<img src="./dot_2.png" alt="Single Complete Graph" style="width:10%;"/>

<p>
Here is the loss curve:
</p>

<img src="./loss_full_complete_5TRIAL.png" alt="Single Complete Graph Loss" style="width:50%;"/>

<p> 
As we can see, it appears extremely noisy. It turns out that the noise is due to multiple different "bands" of loss curves, which is clear if we zoom in and sample a 1000 points:
</p>

<img src="./loss_snapshot_complete_std_1.0.png" alt="Single Complete Graph Loss" style="width:50%;"/>

<p>
In this case, the loss really takes the form of a <a href="https://en.wikipedia.org/wiki/Multivalued_function">"multi function"</a> and the notion of a derivative is essentially useless in its most direct form because the loss may jump between one of many different curves discontinuously! We can fit a linear regression, like above, to get a better estimate of a gradient (like the orange line shown), but this also doesn't work particularly well to capture the curve.
</p>

<h3 id="mnist">Real Application: MNIST</h3>

<h3>Convergence of Linear Regression Method</h3>

<p>
As a first pass I'll use the linear regression method (which is easier to implement because there are multiple alternate ways to implement the Gaussian smoothing approach). On my first pass directly comparing this method to autodiff in the case of a smooth loss, autodiff worked perfectly and the regression method did not work at all. Thus, it is necessary to look at why the latter fails. 
</p>

<h4 id="linear_classifier">Linear Classifier</h4>

<p>
Let's first remove the dependence on a BNN and just use a linear classifier (i.e. a neural network with no activation functions). The architecture I used is as follows:
</p>
<br>
<img src="./mnist_arch.png" style="max-width:80%; width:40%;" alt="MNIST Arch"/>
<br>
<p>
Note that we cannot easily visualize the full loss landscape in this setting. However, we can predict exact gradients using automatic differentiation and compare these to those computed using linear regression. This should allow us to get an idea of how the linear regression method can converge for different sample sizes and standard deviations. 
</p>
<p>
Let's first work on an extremely simple case: let the input to the network be uniformly 1 and the loss function just be the mean output, so we want an output that is as negative as possible. In this case, the loss can be computed directly:
$$z_1 = W_1 \cdot \mathbb{1},$$
$$z_2 = W_2 \cdot z_1,$$
$$L(z_2) = \text{avg}(z_2).$$
Note then that the Jacobian of the loss with respect to \(W_2\) is just a matrix where each column is equal to the average of \(z_2\) (this can be derived by hand by looking at the derivative with respect to an individual weight in \(W_2\)). Here's some empirical proof:
</p>

<br>
<center>
<div style="max-width:50%; width:49%; display:inline-block; ">
<b style="font-size:0.7cm;">Start of Autodiff Learning</b>
<img src="grad_True_0.png" style="max-width:100%;" alt="Start"/>
</div>
<div  style="max-width:50%; width:49%;  display:inline-block;">
<b style="font-size:0.7cm;">End of Autodiff Learning</b>
<img src="grad_True_40.png" style="max-width:100%;" alt="End"/>
</div>
</center>
<br>

<p>
Let's see what happens when we apply the regression method with a very small standard deviation and a specific choice of \(S\). As \(S\) grows very larger (greater than or equal to the number of parameters), the approximation should be almost exactly the same. Below is an example where we isolate the first 10 rows of \(W_2\) and only fit the regression to these parameters, where we set \(S = 500\).
</p>

<img src="first_10_w2_S=500.png" style="max-width:100%; width:70%;" alt="Comparison"/>
<br>

<p>
We can see that the results are almost perfect! One important thing to note that I found is that changing weights in all layers at once with noise gives bad results but when we apply noise to isolated weight layers one at a time, the results are much less noisy. This isn't a big deal, it just means we need to be a bit more careful about we distribute \(S\) between layers. For now, I will just look at fitting the gradient for \(W_2\).
</p>

<p>
To understand what is happening in the above figure, let's also try plot the losses isolated along particular parameters. Below I plotted the losses with respect to the value of a paricular weight. I isolated the weights in the first column and first row, second column and first row, first column and second row. As can be seen, the structure is almost identical for the former two but different for the latter. This explains why the gradient does not change between values in the same row, as expected! 
</p>

<img src="scatters.png" style="max-width:100%; width:70%;" alt="Scatters"/>

<p>
Let's now move to the MNIST benchmark, where things are a little more complex. In this case, instead of the loss being the mean output of the neuron, it is based on the target desired digit and we use MSE loss. As can be seen, with sufficiently many samples, we can perfectly match the gradient: 
</p>

<img src="mnist_perfect_match.png" style="max-width:100%; width:70%;" alt="MNIST Linear Classifier"/>

<p> 
We might now ask how the method converges to the autodiff result for this particular case. As it turns out, in this case we need \(S\) to be greater than or equal to the number of parameters being trained, unforunately. This is clear in the figure below:
</p>

<br>
<img src="fit_s.png" style="max-width:100%; width:70%;" alt="FIT S"/>

<p>
This figure shows the L2 error between the autodiff gradient and the approximate gradient for the output layer isolated only. Since the hidden layer has 100 neurons and output layer has 10, there are 1000 parameters. As can be seen, after 1000 parameters we get good accuracy. I think the spike around 1000 samples may be due to some numerical instability around this value in the method, but I'm not entirely sure. What is most important is that we need a very large and infeasible \(S\) value in this case. Note however that the linear classifier is not a typical case. Because the function is strictly linear, it makes sense that we cannot perform a perfect inversion to get the gradient unless we have a well-conditioned setup. 
</p>

<p>
Although this case may seem disheartening because we need such a large \(S\), for more practical cases I have found this is definitely not the case. To see this, let's move to an artificial neural network (ANN) on MNIST. 
</p>

<h4 id="ann">ANN</h4>

<p>
Let's now see the results with an ANN. In this case, I just inserted a sigmoid layer after the hidden and output layers of the network. Below is an example where we compute the output layer gradient as above. 
</p>

<br>
<img src="ann_result.png" style="max-width:100%; width:70%;" alt="MNIST ANN"/>

<p>
As can be seen, the results are pretty good. Let's now look at convergence:
</p>

<br>
<img src="ann_fit_s.png" style="max-width:100%; width:70%;" alt="ANN Fit S"/>

<p>
It seems like the gradient fitting still is not very good unless we use an \(S\) that approaches the number of parameters of the network. <b>TODO: TEXT HERE</b>.
</p>

<p>
Let's try fitting the network to MNIST now. I used a batch size of 10 and \(S = 100\) and alternated between the input and hidden layers for optimization in the regression method. Alternating like this is important because, as noted above, when we add noise in both layers at once it can skew the observed gradient in the output layer substantially. 
</p>

<p>
Although \(S = 100\) is much lower than the number of parameters in each of the two layers (1000 parameters in \(W_2\) and 78,400 in \(W_1\)), optimization seemed to work pretty comparably well to the autodiff approach:
</p>

<br>
<center>
<div style="max-width:50%; width:49%; display:inline-block; ">
<b style="font-size:0.7cm;">50 batches (500 samples)</b>
<img src="results_100timesteps.png" style="max-width:100%;" alt="50 batches (500 samples)"/>
</div>
<div  style="max-width:50%; width:49%;  display:inline-block;">
<b style="font-size:0.7cm;">500 batches (5000 samples)</b>
<img src="results_1000timesteps.png" style="max-width:100%;" alt="500 batches (5000 samples)"/>
</div>
</center>
<br>

<p>
Success! We finally got something to actually work somewhat correctly! 
</p>

<p>
The next step is to apply these principles to more complex scenarios we are actually interested in. Firstly, we need to prove the method can converge with a BNN in a circumstance where the network IS trainable with the vanilla autodiff method. Next--what we really care about--we need to show that the method can generalize and train the network in a circumstance where a BNN is NOT trainable with autodiff. These two circumstances are actually easy to create:
</p>
<br>
<p>
<b>1.</b> If we use a very small timeframe (e.g. 30ms), autodiff works effectively because only a few numbers of spikes can occur and the loss landscape is effectively smooth, 
</p>
<p>
<b>2.</b> If we use a larger timeframe (e.g. 1 second), autodiff does not work because the loss landscape is very discontinous without a clear notion of derivative (see the many examples of this above). 
</p>

<h4 id="bnn">BNN</h4>

<h4 id="case_1_bnn">Case 1: Small Timeframe</h4>

<p>
First, we will work on case 1 above: simulating a BNN for a very short timeframe. This has the advantage that the loss landscape is quite smooth and autodiff works for training normally. For details on how to train in this regime with MNIST, you can see my first paper (ADD CITATION). 
</p>

<p>
In this circumstance, it turns out that training is quite a lot harder than with the ANN. Let's look at the results. Here's how the predicted second layer (1000 parameters) gradient looks with \(S = 1,100\):
</p>

<br>
<img src="bnn_results_all.png" style="max-width:100%; width:60%;" alt="BNN Results"/>

<p>
As can be seen, even though \(S\) should be sufficiently large, the gradient prediction is terrible! Let's look at how the error depends on \(S\):
</p>

<br>
<img src="bnn_fit_s_1100.png" style="max-width:100%; width:60%;" alt="BNN Fit S"/>

<p>
We can understand the trend before the peak at 1000 (the number of parameters by zooming in:
</p>

<br>
<img src="bnn_fit_s_800.png" style="max-width:100%; width:60%;" alt="BNN Fit S Zoom"/>

<p>
These results are very weird. They seem to suggest that using a smaller \(S\) actually gives better results! 
</p>

<p>
One possible cause for this weird discrepancy could be that the loss I used in these cases was evaluated for a single trial of the BNN. Averaging over samples should potentially fix this as a side effect however. Another possible cause could be our choice of standard deviation. We can see that the method is somewhat succesful with the gradient prediction along the column with nonzero entries above, but there is a lot of noise that should be zero in the other columns. Let's see how the error varies with standard deviation:
</p>

<br>
<img src="varying_std_bnn.png" style="max-width:100%; width:60%;" alt="Vary STD BNN"/>

<p>
From the plots above it appears that using a larger standard deviation actually gives significantly better results! I intuitively understand this as the larger standard deviation makes it so that high frequency noise around the paramter point has less of an impact on the predicted gradient. This can be seen in the plot below, where the only change was using a standard deviation of 0.15:
</p>

<br>
<img src="grads_match.png" style="max-width:100%; width:60%;" alt="Higher STD"/>

<p>
As can be seen, the entries that should be zero are now almost all zero, i.e. there is less of the small noise changing the prediction in the columns where the true gradient is zero. We can see that the gradient along this column is also well matched.
</p>

<h4 id="case_2_bnn">Case 2: Long Timeframe</h4>

<h3 id="signal">Signal Processing Approach</h3>

<p>
This research is very open and I am considering a number of approaches. One that seems like a promising approach is using signal processing methods for training BNNs. This section will explore this.
</p>

<h4>Cross-Correlation</h4>

<p>
For real or complex functions \(f, g\) on some domain \(D\), their cross-correlation is defined as:
$$(f \star g)(\tau) := \int_D \overline{f(t)} g(t + \tau) dt = \int_D \overline{f(t - \tau)} g(t) dt,$$
where \(\overline{f}\) is the complex conjugate and \(\tau\) is called the "lag". To understand this definition, if \(g(t)\) is a discrete signal and is just \(f(t)\) but shifted to the right by \(\tau\), then \(f \star g\) will be zero everywhere except at \(\tau = 1\), where it will be the integral of \(|f(t)|^2\), which is the squared <a href="https://mathworld.wolfram.com/L2-Norm.html">L^2 norm</a> of \(f(t)\). Note that if \(g, f\) are not signals and are continuously varying, then the cross-correlation will not be zero outside but will decay outside 1 and have a max at 1.
</p>

<p>
Cross-correlation loss:
$$\text{Loss} = \int_0^T y(t) y^*(t + \tau) dt.$$
</p>

<h3 id="refs">Useful References</h3>
<ol>
	
    <li><a href="www.gatsby.ucl.ac.uk/~pel/tn/notes/node_perturbation.pdf">Node perturbation in vanilla deep networks</a></li>
    <li><a href="https://arxiv.org/pdf/1706.04698.pdf">Gradient Descent for Spiking Neural Networks</a></li>
    <li><a href="https://arxiv.org/pdf/1901.09948.pdf">Surrogate Gradient Learning in Spiking Neural Networks</a></li>
    <li><a href="https://nowak.ece.wisc.edu/ece830/ece830_spring13_adaptive_filtering.pdf">Adaptive Filtering</a></li>
    <li><a href="https://arxiv.org/pdf/1712.09913.pdf">Visualizing the Loss Landscape of Neural Networks</a></li>
    <li><a href="https://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">Machine Learning Trick of the Day (5): Log Derivative Trick</a></li>
</ol>






<!--<img src="./voltage.png" style="max-width:100%; width:100%;" alt="Voltage"/>-->


<div class="top_bar" style="left: 30px;">
<a href="index.html" style="text-decoration: none;">goto: main</a>
</div>

<div class="top_bar" style="right: 30px;">
<a href="" style="text-decoration: none;">goto: top</a>
</div>

</body>
</html>
